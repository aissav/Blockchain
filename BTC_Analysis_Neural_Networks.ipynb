{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAqhHqUbkVNa"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBBiFhcozpZk"
   },
   "source": [
    "## BTC transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pSKMLmnk1rL"
   },
   "outputs": [],
   "source": [
    "data_matrix = pd.read_csv('Data.csv',names = ['From','To','Rating','Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cS4g2msck6CV",
    "outputId": "b2783c05-596d-4baf-8b71-32988b87e33c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.289242e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.289242e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.289243e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.289245e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1.289254e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   From  To  Rating     Timestamp\n",
       "0     6   2       4  1.289242e+09\n",
       "1     6   5       2  1.289242e+09\n",
       "2     1  15       1  1.289243e+09\n",
       "3     4   3       7  1.289245e+09\n",
       "4    13  16       8  1.289254e+09"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 2, 0.4),\n",
       " (6, 5, 0.2),\n",
       " (1, 15, 0.1),\n",
       " (4, 3, 0.7),\n",
       " (13, 16, 0.8),\n",
       " (13, 10, 0.8),\n",
       " (7, 5, 0.1),\n",
       " (2, 21, 0.5),\n",
       " (2, 20, 0.5),\n",
       " (21, 2, 0.5),\n",
       " (21, 1, 0.8),\n",
       " (21, 10, 0.8),\n",
       " (21, 8, 0.9),\n",
       " (21, 3, 0.7),\n",
       " (17, 3, 0.5),\n",
       " (17, 23, 0.1),\n",
       " (10, 1, 0.8),\n",
       " (10, 6, 0.7),\n",
       " (10, 21, 0.8),\n",
       " (10, 8, 0.1),\n",
       " (10, 25, 1.0),\n",
       " (10, 2, 0.7),\n",
       " (10, 3, 0.7),\n",
       " (4, 26, 0.1),\n",
       " (26, 4, 0.1),\n",
       " (5, 1, 0.3),\n",
       " (5, 6, 0.3),\n",
       " (5, 7, 0.1),\n",
       " (1, 5, 0.4),\n",
       " (6, 4, 0.2),\n",
       " (4, 6, 0.5),\n",
       " (2, 4, 0.5),\n",
       " (17, 28, 0.1),\n",
       " (17, 13, 0.2),\n",
       " (13, 17, 0.2),\n",
       " (13, 29, 0.2),\n",
       " (29, 13, 0.2),\n",
       " (17, 20, 0.2),\n",
       " (4, 31, 0.1),\n",
       " (31, 4, 0.2),\n",
       " (32, 6, 0.1),\n",
       " (13, 1, 0.8),\n",
       " (7, 34, 0.1),\n",
       " (34, 7, 0.1),\n",
       " (32, 1, 0.1),\n",
       " (1, 32, 0.1),\n",
       " (1, 34, 0.1),\n",
       " (34, 1, 0.1),\n",
       " (34, 13, 0.1),\n",
       " (13, 34, 0.1),\n",
       " (6, 7, 0.5),\n",
       " (7, 6, 0.3),\n",
       " (1, 17, 0.9),\n",
       " (1, 31, 0.2),\n",
       " (31, 1, 0.3),\n",
       " (35, 6, 0.2),\n",
       " (1, 13, 0.3),\n",
       " (36, 37, 0.1),\n",
       " (37, 36, 0.1),\n",
       " (35, 1, 0.1),\n",
       " (17, 1, 0.9),\n",
       " (8, 1, 0.3),\n",
       " (7, 29, 0.2),\n",
       " (1, 20, 0.2),\n",
       " (37, 44, 0.1),\n",
       " (44, 37, 0.1),\n",
       " (39, 45, 0.1),\n",
       " (39, 7, 0.3),\n",
       " (39, 44, 0.1),\n",
       " (44, 39, 0.1),\n",
       " (23, 17, 0.1),\n",
       " (23, 19, 0.1),\n",
       " (36, 46, 0.1),\n",
       " (46, 36, 0.1),\n",
       " (47, 1, 0.2),\n",
       " (13, 7, 0.1),\n",
       " (7, 13, 0.1),\n",
       " (29, 51, 0.1),\n",
       " (51, 29, 0.1),\n",
       " (29, 52, 0.1),\n",
       " (52, 29, 0.1),\n",
       " (45, 39, 0.2),\n",
       " (45, 53, 0.2),\n",
       " (53, 45, 0.2),\n",
       " (45, 54, 0.2),\n",
       " (54, 45, 0.2),\n",
       " (1, 54, 0.1),\n",
       " (54, 1, 0.1),\n",
       " (7, 3, 0.6),\n",
       " (36, 41, 0.2),\n",
       " (41, 36, 0.2),\n",
       " (36, 21, 0.2),\n",
       " (21, 36, 0.2),\n",
       " (7, 36, 0.3),\n",
       " (36, 7, 0.4),\n",
       " (7, 53, 0.4),\n",
       " (53, 7, 0.4),\n",
       " (7, 55, 0.1),\n",
       " (55, 7, 0.1),\n",
       " (29, 7, 0.2),\n",
       " (4, 57, 0.5),\n",
       " (39, 21, 0.2),\n",
       " (21, 39, 0.2),\n",
       " (4, 61, 0.2),\n",
       " (61, 4, 0.3),\n",
       " (61, 20, 0.8),\n",
       " (61, 2, 0.2),\n",
       " (35, 65, 0.1),\n",
       " (65, 35, 0.2),\n",
       " (56, 1, 0.2),\n",
       " (1, 56, 0.2),\n",
       " (4, 66, 0.2),\n",
       " (66, 4, 0.2),\n",
       " (21, 7, 0.5),\n",
       " (7, 20, 0.4),\n",
       " (35, 69, 0.1),\n",
       " (35, 70, 0.1),\n",
       " (39, 69, 0.2),\n",
       " (69, 39, 0.2),\n",
       " (69, 35, 0.2),\n",
       " (1, 71, 0.1),\n",
       " (71, 1, 0.8),\n",
       " (7, 28, 0.2),\n",
       " (28, 7, 0.1),\n",
       " (4, 72, 0.4),\n",
       " (1, 74, 0.1),\n",
       " (74, 1, 0.1),\n",
       " (41, 75, 0.1),\n",
       " (68, 64, 0.1),\n",
       " (76, 7, 0.1),\n",
       " (7, 76, 0.1),\n",
       " (7, 19, 0.5),\n",
       " (32, 23, 0.1),\n",
       " (23, 32, 0.1),\n",
       " (23, 20, 0.1),\n",
       " (61, 77, 0.4),\n",
       " (1, 78, 0.1),\n",
       " (78, 1, 0.1),\n",
       " (60, 41, 0.1),\n",
       " (41, 60, 0.1),\n",
       " (78, 60, 0.1),\n",
       " (60, 78, 0.1),\n",
       " (57, 4, 0.2),\n",
       " (23, 69, 0.1),\n",
       " (69, 23, 0.1),\n",
       " (64, 68, 0.1),\n",
       " (35, 79, 0.1),\n",
       " (79, 35, 0.1),\n",
       " (41, 4, 0.1),\n",
       " (4, 41, 0.1),\n",
       " (1, 68, 0.2),\n",
       " (68, 1, 0.2),\n",
       " (69, 28, 0.3),\n",
       " (28, 69, 0.3),\n",
       " (64, 80, 0.1),\n",
       " (17, 81, 0.1),\n",
       " (81, 17, 0.1),\n",
       " (69, 62, 0.1),\n",
       " (62, 69, 0.1),\n",
       " (41, 83, 0.1),\n",
       " (41, 104, 0.1),\n",
       " (83, 41, 0.1),\n",
       " (41, 86, 0.1),\n",
       " (86, 41, 0.1),\n",
       " (104, 41, 0.1),\n",
       " (4, 87, 0.1),\n",
       " (87, 4, 0.1),\n",
       " (81, 69, 0.2),\n",
       " (69, 81, 0.2),\n",
       " (41, 89, 0.1),\n",
       " (89, 41, 0.1),\n",
       " (60, 87, 0.1),\n",
       " (87, 60, 0.1),\n",
       " (7, 89, 0.2),\n",
       " (89, 7, 0.2),\n",
       " (64, 94, 0.1),\n",
       " (41, 70, 0.1),\n",
       " (94, 64, 0.1),\n",
       " (70, 95, 0.1),\n",
       " (95, 70, 0.3),\n",
       " (70, 41, 0.1),\n",
       " (70, 35, 0.1),\n",
       " (39, 17, 0.3),\n",
       " (17, 39, 0.1),\n",
       " (70, 96, 0.1),\n",
       " (96, 70, 0.3),\n",
       " (15, 97, 0.2),\n",
       " (97, 15, 0.2),\n",
       " (69, 99, 0.4),\n",
       " (99, 69, 0.1),\n",
       " (88, 7, 0.3),\n",
       " (1, 81, 0.1),\n",
       " (70, 15, 0.1),\n",
       " (15, 70, 0.1),\n",
       " (81, 1, 0.1),\n",
       " (1, 101, 0.1),\n",
       " (62, 54, 0.2),\n",
       " (54, 62, 0.2),\n",
       " (101, 1, 0.1),\n",
       " (100, 101, 0.1),\n",
       " (101, 103, 0.1),\n",
       " (80, 25, 0.2),\n",
       " (100, 60, 0.2),\n",
       " (39, 4, 0.1),\n",
       " (4, 39, 0.1),\n",
       " (41, 105, 0.1),\n",
       " (72, 4, 0.4),\n",
       " (96, 106, 0.1),\n",
       " (106, 96, 0.1),\n",
       " (81, 107, 0.2),\n",
       " (107, 81, 0.3),\n",
       " (60, 108, 0.1),\n",
       " (108, 60, 0.1),\n",
       " (97, 68, 0.1),\n",
       " (68, 97, 0.1),\n",
       " (96, 107, 0.2),\n",
       " (7, 109, 0.1),\n",
       " (109, 7, 0.1),\n",
       " (41, 110, 0.1),\n",
       " (41, 68, 0.1),\n",
       " (68, 41, 0.1),\n",
       " (107, 96, 0.2),\n",
       " (23, 7, 0.5),\n",
       " (7, 111, 0.1),\n",
       " (111, 7, 0.1),\n",
       " (64, 112, 0.2),\n",
       " (112, 64, 0.2),\n",
       " (112, 77, 0.1),\n",
       " (112, 115, 0.2),\n",
       " (112, 60, 0.4),\n",
       " (60, 112, 0.4),\n",
       " (109, 39, 0.1),\n",
       " (39, 109, 0.1),\n",
       " (6, 114, 0.2),\n",
       " (114, 39, 0.1),\n",
       " (6, 32, 0.1),\n",
       " (39, 114, 0.1),\n",
       " (47, 7, 0.1),\n",
       " (1, 119, 0.1),\n",
       " (119, 1, 1.0),\n",
       " (100, 1, 0.3),\n",
       " (100, 7, 0.4),\n",
       " (7, 100, 0.2),\n",
       " (7, 47, 0.1),\n",
       " (7, 122, 0.1),\n",
       " (122, 7, 0.1),\n",
       " (23, 108, 0.1),\n",
       " (108, 23, 0.1),\n",
       " (108, 104, 0.1),\n",
       " (104, 108, 0.1),\n",
       " (60, 56, 0.2),\n",
       " (115, 77, 0.3),\n",
       " (119, 127, 1.0),\n",
       " (119, 25, 0.1),\n",
       " (1, 2, 0.8),\n",
       " (1, 76, 0.2),\n",
       " (76, 1, 0.2),\n",
       " (1, 23, 0.6),\n",
       " (54, 129, 0.1),\n",
       " (116, 7, 0.2),\n",
       " (7, 116, 0.2),\n",
       " (54, 13, 0.2),\n",
       " (13, 54, 0.2),\n",
       " (77, 39, 0.3),\n",
       " (77, 15, 0.1),\n",
       " (77, 6, 0.3),\n",
       " (77, 1, 0.3),\n",
       " (77, 23, 0.1),\n",
       " (77, 28, 0.3),\n",
       " (77, 115, 0.3),\n",
       " (77, 112, 0.1),\n",
       " (77, 131, 0.1),\n",
       " (77, 61, 0.3),\n",
       " (129, 54, 0.1),\n",
       " (54, 2, 0.2),\n",
       " (2, 54, 0.2),\n",
       " (127, 119, 1.0),\n",
       " (127, 132, 0.1),\n",
       " (132, 127, 0.2),\n",
       " (7, 133, 0.1),\n",
       " (133, 7, 0.1),\n",
       " (105, 41, 0.1),\n",
       " (41, 64, 0.1),\n",
       " (64, 41, 0.1),\n",
       " (57, 110, 0.1),\n",
       " (113, 125, 0.2),\n",
       " (125, 113, 0.2),\n",
       " (54, 7, 0.6),\n",
       " (7, 54, 0.5),\n",
       " (113, 23, 0.2),\n",
       " (113, 135, 0.2),\n",
       " (135, 113, 0.2),\n",
       " (78, 36, 0.2),\n",
       " (36, 78, 0.2),\n",
       " (2, 132, 0.2),\n",
       " (132, 2, 0.2),\n",
       " (96, 15, 0.4),\n",
       " (110, 57, 0.1),\n",
       " (110, 41, 0.1),\n",
       " (132, 28, 0.1),\n",
       " (28, 132, 0.1),\n",
       " (111, 1, 0.1),\n",
       " (132, 134, 0.1),\n",
       " (134, 132, 0.1),\n",
       " (132, 100, 0.1),\n",
       " (100, 132, 0.1),\n",
       " (41, 137, 0.1),\n",
       " (15, 96, 0.3),\n",
       " (137, 41, 0.1),\n",
       " (7, 137, 0.1),\n",
       " (137, 7, 0.1),\n",
       " (1, 138, 0.1),\n",
       " (138, 1, 0.1),\n",
       " (132, 138, 0.1),\n",
       " (138, 132, 0.1),\n",
       " (100, 111, 0.1),\n",
       " (111, 100, 0.1),\n",
       " (100, 64, 0.1),\n",
       " (64, 100, 0.1),\n",
       " (41, 140, 0.1),\n",
       " (140, 41, 0.1),\n",
       " (7, 60, 0.5),\n",
       " (60, 100, 0.2),\n",
       " (7, 141, 0.2),\n",
       " (141, 134, 0.5),\n",
       " (134, 141, 0.5),\n",
       " (141, 7, 0.3),\n",
       " (132, 21, 0.5),\n",
       " (139, 132, 0.1),\n",
       " (132, 139, 0.1),\n",
       " (78, 132, 0.3),\n",
       " (2, 3, 0.8),\n",
       " (132, 142, 0.1),\n",
       " (142, 132, 0.1),\n",
       " (110, 142, 0.1),\n",
       " (1, 110, 0.1),\n",
       " (110, 1, 0.1),\n",
       " (142, 110, 0.1),\n",
       " (110, 76, 0.2),\n",
       " (2, 142, 0.1),\n",
       " (142, 2, 0.1),\n",
       " (110, 143, 0.1),\n",
       " (143, 110, 0.1),\n",
       " (75, 132, 0.1),\n",
       " (110, 134, 0.1),\n",
       " (134, 110, 0.1),\n",
       " (132, 144, 0.1),\n",
       " (144, 132, 0.1),\n",
       " (1, 145, 0.1),\n",
       " (145, 1, 0.1),\n",
       " (146, 2, 0.1),\n",
       " (60, 147, 0.1),\n",
       " (41, 148, 0.1),\n",
       " (148, 41, 0.1),\n",
       " (23, 113, 0.4),\n",
       " (21, 132, 0.2),\n",
       " (93, 81, 0.2),\n",
       " (78, 149, 0.1),\n",
       " (149, 150, 0.1),\n",
       " (150, 149, 0.1),\n",
       " (132, 75, 0.1),\n",
       " (2, 110, 0.1),\n",
       " (110, 2, 0.1),\n",
       " (110, 144, 0.1),\n",
       " (144, 110, 0.1),\n",
       " (41, 147, 0.1),\n",
       " (147, 41, 0.1),\n",
       " (132, 152, 0.1),\n",
       " (104, 153, 0.2),\n",
       " (81, 93, 0.3),\n",
       " (152, 154, 0.1),\n",
       " (154, 152, 0.2),\n",
       " (152, 75, 0.1),\n",
       " (110, 155, 0.1),\n",
       " (132, 156, 0.4),\n",
       " (156, 132, 0.4),\n",
       " (132, 113, 0.3),\n",
       " (113, 132, 0.3),\n",
       " (155, 110, 0.1),\n",
       " (41, 157, 0.1),\n",
       " (157, 41, 0.1),\n",
       " (149, 132, 0.1),\n",
       " (132, 149, 0.1),\n",
       " (113, 158, 0.1),\n",
       " (158, 113, 0.2),\n",
       " (56, 7, 0.3),\n",
       " (7, 56, 0.3),\n",
       " (41, 132, 0.1),\n",
       " (153, 104, 0.2),\n",
       " (132, 41, 0.1),\n",
       " (114, 1, 0.2),\n",
       " (1, 114, 0.2),\n",
       " (1, 111, 0.2),\n",
       " (60, 160, 0.1),\n",
       " (160, 60, 0.1),\n",
       " (114, 149, 0.1),\n",
       " (149, 114, 0.1),\n",
       " (114, 146, 0.1),\n",
       " (78, 146, 0.1),\n",
       " (146, 78, 0.1),\n",
       " (122, 78, 0.1),\n",
       " (132, 78, 0.3),\n",
       " (146, 114, 0.1),\n",
       " (114, 21, 0.1),\n",
       " (7, 161, 0.3),\n",
       " (161, 7, 0.3),\n",
       " (21, 114, 0.1),\n",
       " (132, 104, 0.1),\n",
       " (104, 132, 0.1),\n",
       " (132, 157, 0.2),\n",
       " (157, 132, 0.2),\n",
       " (1, 7, 0.9),\n",
       " (7, 1, 0.7),\n",
       " (132, 162, 0.1),\n",
       " (162, 132, 0.1),\n",
       " (41, 163, 0.1),\n",
       " (163, 41, 0.1),\n",
       " (41, 162, 0.1),\n",
       " (162, 41, 0.1),\n",
       " (2, 149, 0.1),\n",
       " (149, 2, 0.1),\n",
       " (75, 41, 0.2),\n",
       " (143, 81, 0.1),\n",
       " (143, 80, 0.1),\n",
       " (80, 143, 0.1),\n",
       " (1, 165, 0.1),\n",
       " (165, 1, 0.1),\n",
       " (152, 166, 0.1),\n",
       " (166, 152, 0.1),\n",
       " (2, 166, 0.1),\n",
       " (166, 2, 0.1),\n",
       " (110, 104, 0.1),\n",
       " (41, 167, 0.1),\n",
       " (167, 41, 0.1),\n",
       " (41, 168, 0.1),\n",
       " (168, 41, 0.1),\n",
       " (132, 169, 0.2),\n",
       " (169, 132, 0.2),\n",
       " (81, 149, 0.1),\n",
       " (142, 170, 0.1),\n",
       " (170, 142, 0.1),\n",
       " (45, 171, 0.2),\n",
       " (171, 45, 0.2),\n",
       " (64, 166, 0.1),\n",
       " (166, 64, 0.1),\n",
       " (113, 166, 0.1),\n",
       " (166, 113, 0.1),\n",
       " (60, 104, 0.2),\n",
       " (60, 172, 0.1),\n",
       " (64, 3, 0.1),\n",
       " (113, 173, 0.2),\n",
       " (173, 113, 0.1),\n",
       " (41, 174, 0.1),\n",
       " (172, 60, 0.1),\n",
       " (166, 156, 0.2),\n",
       " (156, 166, 0.2),\n",
       " (175, 7, 0.2),\n",
       " (45, 176, 0.3),\n",
       " (161, 144, 0.1),\n",
       " (144, 161, 0.1),\n",
       " (7, 166, 0.1),\n",
       " (161, 177, 0.1),\n",
       " (177, 161, 0.1),\n",
       " (149, 129, 0.1),\n",
       " (129, 149, 0.1),\n",
       " (144, 178, 0.1),\n",
       " (178, 144, 0.1),\n",
       " (75, 1, 0.2),\n",
       " (75, 179, 0.1),\n",
       " (179, 75, 0.1),\n",
       " (75, 62, 0.2),\n",
       " (62, 75, 0.2),\n",
       " (143, 173, 0.1),\n",
       " (149, 173, 0.1),\n",
       " (173, 149, 0.1),\n",
       " (173, 143, 0.1),\n",
       " (166, 180, 0.1),\n",
       " (180, 166, 0.1),\n",
       " (174, 41, 0.1),\n",
       " (149, 81, 0.3),\n",
       " (144, 143, 0.1),\n",
       " (143, 144, 0.1),\n",
       " (60, 148, 0.2),\n",
       " (148, 60, 0.2),\n",
       " (2, 156, 0.1),\n",
       " (156, 2, 0.1),\n",
       " (1, 19, 0.6),\n",
       " (132, 181, 0.1),\n",
       " (181, 132, 0.1),\n",
       " (135, 181, 0.1),\n",
       " (181, 135, 0.1),\n",
       " (132, 159, 0.1),\n",
       " (159, 132, 0.1),\n",
       " (135, 143, 0.1),\n",
       " (143, 135, 0.1),\n",
       " (148, 182, 0.1),\n",
       " (182, 148, 0.1),\n",
       " (144, 181, 0.1),\n",
       " (181, 144, 0.1),\n",
       " (181, 183, 0.2),\n",
       " (183, 181, 0.2),\n",
       " (143, 183, 0.1),\n",
       " (183, 143, 0.1),\n",
       " (142, 64, 0.1),\n",
       " (64, 142, 0.1),\n",
       " (184, 7, 0.2),\n",
       " (7, 184, 0.2),\n",
       " (80, 7, 0.1),\n",
       " (7, 78, 0.1),\n",
       " (78, 7, 0.1),\n",
       " (147, 179, 0.1),\n",
       " (179, 147, 0.1),\n",
       " (7, 185, 0.1),\n",
       " (185, 7, 0.1),\n",
       " (176, 45, 0.1),\n",
       " (183, 186, 0.1),\n",
       " (186, 183, 0.1),\n",
       " (135, 187, 0.1),\n",
       " (187, 135, 0.1),\n",
       " (33, 7, 0.2),\n",
       " (7, 33, 0.2),\n",
       " (78, 144, 0.3),\n",
       " (7, 188, 0.1),\n",
       " (188, 7, 0.1),\n",
       " (104, 189, 0.2),\n",
       " (189, 104, 0.2),\n",
       " (104, 190, 0.1),\n",
       " (190, 104, 0.1),\n",
       " (104, 110, 0.2),\n",
       " (104, 191, 0.1),\n",
       " (191, 104, 0.1),\n",
       " (104, 113, 0.1),\n",
       " (113, 104, 0.1),\n",
       " (6, 173, 0.3),\n",
       " (110, 7, 0.1),\n",
       " (104, 171, 0.1),\n",
       " (171, 104, 0.1),\n",
       " (143, 192, 0.1),\n",
       " (192, 143, 0.1),\n",
       " (60, 159, 0.2),\n",
       " (159, 60, 0.2),\n",
       " (2, 1, 0.8),\n",
       " (57, 25, 0.2),\n",
       " (60, 178, 0.2),\n",
       " (7, 35, 0.2),\n",
       " (60, 193, 0.1),\n",
       " (193, 60, 0.1),\n",
       " (60, 194, 0.1),\n",
       " (194, 178, 0.1),\n",
       " (194, 60, 0.1),\n",
       " (178, 194, 0.1),\n",
       " (7, 195, 0.1),\n",
       " (195, 7, 0.1),\n",
       " (29, 60, 0.2),\n",
       " (62, 196, 0.1),\n",
       " (196, 62, 0.1),\n",
       " (60, 29, 0.2),\n",
       " (60, 191, 0.4),\n",
       " (7, 156, 0.1),\n",
       " (156, 7, 0.1),\n",
       " (60, 198, 0.1),\n",
       " (60, 199, 0.1),\n",
       " (199, 60, 0.1),\n",
       " (7, 200, 0.2),\n",
       " (200, 7, 0.2),\n",
       " (132, 198, 0.2),\n",
       " (198, 132, 0.1),\n",
       " (35, 7, 0.2),\n",
       " (194, 201, 0.1),\n",
       " (201, 194, 0.1),\n",
       " (100, 202, 0.1),\n",
       " (202, 100, 0.1),\n",
       " (142, 203, 0.1),\n",
       " (203, 142, 0.1),\n",
       " (7, 204, 0.1),\n",
       " (142, 60, 0.1),\n",
       " (7, 110, 0.3),\n",
       " (60, 205, 0.1),\n",
       " (206, 201, 0.1),\n",
       " (201, 206, 0.1),\n",
       " (207, 7, 0.2),\n",
       " (200, 62, 0.1),\n",
       " (60, 75, 0.3),\n",
       " (75, 60, 0.3),\n",
       " (75, 7, 0.3),\n",
       " (60, 209, 0.3),\n",
       " (7, 210, 0.1),\n",
       " (210, 7, 0.1),\n",
       " (142, 143, 0.2),\n",
       " (143, 142, 0.2),\n",
       " (60, 142, 0.1),\n",
       " (7, 206, 0.2),\n",
       " (149, 209, 0.1),\n",
       " (120, 7, 0.5),\n",
       " (7, 207, 0.3),\n",
       " (7, 96, 0.1),\n",
       " (104, 179, -0.1),\n",
       " (200, 179, -0.1),\n",
       " (149, 179, -0.1),\n",
       " (96, 7, 0.2),\n",
       " (56, 60, 0.2),\n",
       " (202, 211, 0.1),\n",
       " (200, 13, 0.1),\n",
       " (13, 200, 0.1),\n",
       " (211, 202, 0.1),\n",
       " (104, 149, 0.1),\n",
       " (149, 104, 0.1),\n",
       " (104, 60, 0.3),\n",
       " (149, 62, 0.1),\n",
       " (167, 211, 0.1),\n",
       " (206, 7, 0.2),\n",
       " (202, 212, 0.1),\n",
       " (212, 202, 0.1),\n",
       " (212, 209, 0.1),\n",
       " (60, 212, 0.4),\n",
       " (212, 60, 0.4),\n",
       " (101, 134, 0.1),\n",
       " (60, 213, 0.2),\n",
       " (213, 60, 0.2),\n",
       " (134, 101, 0.1),\n",
       " (212, 7, 0.1),\n",
       " (212, 132, 0.1),\n",
       " (212, 29, 0.1),\n",
       " (143, 153, 0.1),\n",
       " (153, 143, 0.2),\n",
       " (35, 110, 0.2),\n",
       " (135, 214, 0.1),\n",
       " (143, 215, 0.1),\n",
       " (215, 143, 0.1),\n",
       " (29, 212, 0.1),\n",
       " (7, 212, 0.1),\n",
       " (135, 152, 0.1),\n",
       " (7, 179, -0.1),\n",
       " (39, 149, 0.4),\n",
       " (149, 39, 0.4),\n",
       " (214, 216, 0.1),\n",
       " (216, 214, 0.1),\n",
       " (174, 112, 0.1),\n",
       " (153, 202, 0.1),\n",
       " (29, 120, 0.2),\n",
       " (60, 217, 0.2),\n",
       " (60, 218, 0.1),\n",
       " (218, 60, 0.1),\n",
       " (209, 212, 0.1),\n",
       " (217, 60, 0.2),\n",
       " (217, 3, 0.5),\n",
       " (135, 179, -0.1),\n",
       " (176, 219, 0.1),\n",
       " (219, 176, 0.1),\n",
       " (219, 110, 0.1),\n",
       " (176, 220, 0.1),\n",
       " (1, 223, 0.7),\n",
       " (60, 1, 0.7),\n",
       " (60, 7, 0.7),\n",
       " (39, 2, 0.8),\n",
       " (202, 176, 0.1),\n",
       " (176, 202, 0.1),\n",
       " (60, 224, 0.4),\n",
       " (1, 225, 0.1),\n",
       " (225, 1, 0.1),\n",
       " (225, 209, 0.1),\n",
       " (29, 1, 0.4),\n",
       " (29, 219, 0.1),\n",
       " (219, 29, 0.1),\n",
       " (1, 132, 0.1),\n",
       " (1, 227, 0.1),\n",
       " (227, 1, 0.1),\n",
       " (60, 228, 0.1),\n",
       " (225, 45, 0.1),\n",
       " (60, 214, 0.2),\n",
       " (199, 185, 0.1),\n",
       " (185, 199, 0.1),\n",
       " (60, 230, 0.1),\n",
       " (60, 231, 0.1),\n",
       " (231, 60, 0.1),\n",
       " (7, 232, 0.3),\n",
       " (1, 104, 0.1),\n",
       " (104, 1, 0.2),\n",
       " (33, 1, 0.8),\n",
       " (62, 149, 0.1),\n",
       " (29, 171, 0.1),\n",
       " (206, 233, 0.1),\n",
       " (233, 206, 0.1),\n",
       " (232, 7, 0.2),\n",
       " (60, 208, 0.3),\n",
       " (208, 60, 0.3),\n",
       " (206, 234, 0.1),\n",
       " (234, 206, 0.1),\n",
       " (235, 7, 0.5),\n",
       " (222, 7, 0.2),\n",
       " (104, 202, 0.3),\n",
       " (202, 104, 0.3),\n",
       " (202, 236, 0.1),\n",
       " (236, 202, 0.1),\n",
       " (202, 237, 0.1),\n",
       " (237, 202, 1.0),\n",
       " (149, 238, 0.1),\n",
       " (238, 149, 0.1),\n",
       " (39, 239, 0.1),\n",
       " (239, 64, 0.1),\n",
       " (239, 39, 0.1),\n",
       " (202, 135, 0.2),\n",
       " (135, 202, 0.2),\n",
       " (2, 7, 0.5),\n",
       " (2, 60, 0.5),\n",
       " (2, 62, 0.8),\n",
       " (2, 77, 0.2),\n",
       " (7, 2, 0.5),\n",
       " (7, 39, 0.5),\n",
       " (7, 41, 0.5),\n",
       " (7, 21, 0.5),\n",
       " (7, 72, 0.2),\n",
       " (7, 62, 0.6),\n",
       " (208, 62, 0.1),\n",
       " (206, 240, 0.1),\n",
       " (240, 206, 0.2),\n",
       " (62, 208, 0.1),\n",
       " (231, 241, 0.1),\n",
       " (39, 143, 0.2),\n",
       " (143, 39, 0.2),\n",
       " (241, 231, 0.1),\n",
       " (60, 242, 0.6),\n",
       " (242, 60, 0.6),\n",
       " (135, 2, 0.1),\n",
       " (204, 7, 0.1),\n",
       " (1, 64, 0.3),\n",
       " (64, 1, 0.3),\n",
       " (202, 244, 0.1),\n",
       " (244, 60, 0.2),\n",
       " (202, 56, 0.1),\n",
       " (56, 202, 0.1),\n",
       " (164, 60, 0.5),\n",
       " (60, 164, 0.5),\n",
       " (204, 80, 0.1),\n",
       " (56, 114, 0.1),\n",
       " (149, 80, 0.5),\n",
       " (1, 245, 0.1),\n",
       " (241, 80, 0.3),\n",
       " (35, 246, 0.1),\n",
       " (246, 35, 0.1),\n",
       " (1, 247, 0.1),\n",
       " (247, 1, 0.1),\n",
       " (80, 241, 0.3),\n",
       " (80, 64, 0.1),\n",
       " (80, 204, 0.1),\n",
       " (80, 149, 0.5),\n",
       " (80, 81, 0.8),\n",
       " (228, 60, 0.2),\n",
       " (72, 21, 0.1),\n",
       " (35, 248, 0.2),\n",
       " (248, 35, 0.2),\n",
       " (244, 245, 0.2),\n",
       " (245, 244, 0.2),\n",
       " (245, 250, 0.1),\n",
       " (250, 245, 0.1),\n",
       " (60, 251, 0.1),\n",
       " (60, 250, 0.3),\n",
       " (250, 60, 0.3),\n",
       " (7, 175, 0.2),\n",
       " (166, 206, 0.1),\n",
       " (206, 166, 0.1),\n",
       " (60, 244, 0.4),\n",
       " (166, 158, 0.1),\n",
       " (215, 249, 0.1),\n",
       " (249, 215, 0.1),\n",
       " (60, 243, 0.5),\n",
       " (243, 60, 0.6),\n",
       " (60, 252, 0.1),\n",
       " (252, 60, 0.1),\n",
       " (253, 95, 0.1),\n",
       " (143, 7, 0.5),\n",
       " (7, 143, 0.4),\n",
       " (1, 234, 0.1),\n",
       " (234, 1, 0.1),\n",
       " (251, 245, 0.1),\n",
       " (245, 251, 0.1),\n",
       " (60, 88, 0.2),\n",
       " (88, 60, 0.4),\n",
       " (88, 104, 0.3),\n",
       " (104, 88, 0.3),\n",
       " (60, 255, 0.1),\n",
       " (255, 60, 0.1),\n",
       " (29, 251, 0.2),\n",
       " (166, 7, 0.3),\n",
       " (251, 29, 0.2),\n",
       " (251, 60, 0.5),\n",
       " (251, 1, 0.5),\n",
       " (251, 202, 0.5),\n",
       " (1, 251, 0.2),\n",
       " (164, 163, 0.1),\n",
       " (206, 256, 0.1),\n",
       " (256, 206, 0.1),\n",
       " (60, 257, 1.0),\n",
       " (6, 258, 0.1),\n",
       " (258, 6, 0.1),\n",
       " (64, 219, 0.1),\n",
       " (258, 259, 0.1),\n",
       " (259, 258, 0.2),\n",
       " (1, 260, 0.1),\n",
       " (7, 260, 0.4),\n",
       " (1, 261, 0.1),\n",
       " (261, 1, 0.1),\n",
       " (202, 158, 0.1),\n",
       " (256, 153, 0.1),\n",
       " (202, 240, 0.2),\n",
       " (240, 202, 0.2),\n",
       " (244, 113, 0.1),\n",
       " (244, 114, 0.1),\n",
       " (207, 262, 0.1),\n",
       " (262, 207, 0.1),\n",
       " (23, 263, 0.2),\n",
       " (60, 264, 0.4),\n",
       " (60, 265, 0.4),\n",
       " (265, 60, 0.4),\n",
       " (149, 266, 0.1),\n",
       " (266, 149, 0.1),\n",
       " (35, 267, 0.1),\n",
       " (267, 35, 0.1),\n",
       " (202, 141, 0.1),\n",
       " (141, 202, 0.1),\n",
       " (6, 268, 0.3),\n",
       " (266, 60, 0.5),\n",
       " (229, 215, 0.1),\n",
       " (215, 229, 0.1),\n",
       " (141, 41, 0.1),\n",
       " (242, 141, 0.1),\n",
       " (141, 242, 0.1),\n",
       " (215, 221, 0.2),\n",
       " (268, 6, 0.3),\n",
       " (221, 215, 0.2),\n",
       " (221, 269, 0.1),\n",
       " (269, 221, 0.1),\n",
       " (269, 3, 0.1),\n",
       " (41, 141, 0.1),\n",
       " (60, 269, 0.1),\n",
       " (153, 7, 0.1),\n",
       " (249, 167, 0.1),\n",
       " (221, 171, 0.1),\n",
       " (1, 109, 0.1),\n",
       " (202, 230, 0.1),\n",
       " (171, 221, 0.1),\n",
       " (171, 235, 0.2),\n",
       " (235, 171, 0.1),\n",
       " (64, 7, 0.3),\n",
       " (177, 1, 0.4),\n",
       " (60, 77, 0.4),\n",
       " (149, 25, 1.0),\n",
       " (1, 271, 0.1),\n",
       " (271, 1, 0.1),\n",
       " (221, 272, 0.1),\n",
       " (272, 221, 0.1),\n",
       " (202, 153, 0.4),\n",
       " (60, 261, 0.2),\n",
       " (261, 60, 0.2),\n",
       " (60, 273, 0.2),\n",
       " (23, 231, 0.1),\n",
       " (7, 243, 0.4),\n",
       " (230, 202, 0.2),\n",
       " (144, 78, 0.5),\n",
       " (144, 153, 0.1),\n",
       " (269, 7, 0.1),\n",
       " (7, 269, 0.1),\n",
       " (1, 274, 0.1),\n",
       " (274, 1, 0.1),\n",
       " (141, 1, 0.2),\n",
       " (1, 141, 0.1),\n",
       " (141, 273, 0.1),\n",
       " (141, 275, 0.1),\n",
       " (60, 276, 0.3),\n",
       " (276, 60, 0.3),\n",
       " (141, 256, 0.1),\n",
       " (270, 277, 0.1),\n",
       " (256, 141, 0.1),\n",
       " (277, 270, 0.1),\n",
       " (202, 127, 0.1),\n",
       " (167, 249, 0.1),\n",
       " (275, 141, 0.1),\n",
       " (221, 202, 0.4),\n",
       " (202, 221, 0.4),\n",
       " (221, 64, 0.1),\n",
       " (206, 221, 0.1),\n",
       " (221, 206, 0.1),\n",
       " (273, 60, 0.3),\n",
       " (60, 279, 0.2),\n",
       " (219, 64, 0.5),\n",
       " (219, 109, 0.1),\n",
       " (219, 21, 0.5),\n",
       " (279, 60, 0.8),\n",
       " (202, 278, 0.3),\n",
       " (278, 202, 0.3),\n",
       " (206, 29, 0.1),\n",
       " (29, 206, 0.1),\n",
       " (280, 281, 0.1),\n",
       " (206, 282, 0.1),\n",
       " (242, 274, 0.1),\n",
       " (282, 206, 0.1),\n",
       " (283, 284, 1.0),\n",
       " (7, 240, 0.2),\n",
       " (7, 285, 0.1),\n",
       " (202, 286, 0.1),\n",
       " (286, 202, 0.1),\n",
       " (256, 287, 0.1),\n",
       " (287, 256, 0.1),\n",
       " (202, 288, 0.1),\n",
       " (288, 202, 0.1),\n",
       " (257, 265, 0.4),\n",
       " (257, 242, 0.5),\n",
       " (257, 104, 0.5),\n",
       " (257, 60, 0.5),\n",
       " (202, 134, 0.2),\n",
       " (134, 202, 0.1),\n",
       " (37, 219, 0.2),\n",
       " (219, 37, 0.2),\n",
       " (290, 215, 0.1),\n",
       " (215, 290, 0.1),\n",
       " (284, 283, 1.0),\n",
       " (245, 291, 0.1),\n",
       " (291, 245, 0.1),\n",
       " (292, 202, 0.1),\n",
       " (217, 149, 0.1),\n",
       " (149, 217, 0.1),\n",
       " (224, 257, 0.4),\n",
       " (224, 60, 0.5),\n",
       " (224, 144, 0.1),\n",
       " (144, 224, 0.1),\n",
       " (224, 171, 0.1),\n",
       " (171, 224, 0.1),\n",
       " (135, 293, 0.1),\n",
       " (21, 60, 0.5),\n",
       " (199, 104, 0.1),\n",
       " (7, 294, 0.1),\n",
       " (294, 7, 0.1),\n",
       " (293, 204, 0.1),\n",
       " (204, 293, 0.1),\n",
       " (231, 28, 0.1),\n",
       " (274, 295, 0.2),\n",
       " (295, 274, 0.2),\n",
       " (202, 292, 0.1),\n",
       " (249, 64, 0.1),\n",
       " (230, 101, 0.2),\n",
       " (1, 296, 0.1),\n",
       " (64, 249, 0.1),\n",
       " (229, 297, 0.1),\n",
       " (297, 229, 0.1),\n",
       " (4, 282, 0.2),\n",
       " (282, 4, 0.2),\n",
       " (298, 299, 0.1),\n",
       " (299, 298, 0.1),\n",
       " (257, 29, 0.5),\n",
       " (271, 202, 0.1),\n",
       " (202, 271, 0.1),\n",
       " (272, 7, 0.2),\n",
       " (7, 272, 0.2),\n",
       " (37, 301, 0.1),\n",
       " (301, 37, 0.1),\n",
       " (60, 299, 0.2),\n",
       " (299, 60, 0.3),\n",
       " (29, 302, 0.1),\n",
       " (135, 303, 0.1),\n",
       " (303, 135, 0.1),\n",
       " (64, 301, 0.1),\n",
       " (301, 64, 0.1),\n",
       " (214, 60, 0.3),\n",
       " (29, 304, 0.1),\n",
       " (304, 29, 0.1),\n",
       " (7, 254, 0.1),\n",
       " (254, 7, 0.1),\n",
       " (17, 254, 0.1),\n",
       " (7, 135, 0.2),\n",
       " (284, 21, 0.1),\n",
       " (282, 305, 0.1),\n",
       " (305, 282, 0.1),\n",
       " (282, 306, 0.1),\n",
       " (306, 282, 0.1),\n",
       " (60, 229, 0.2),\n",
       " (229, 60, 0.2),\n",
       " (144, 282, 0.1),\n",
       " (282, 144, 0.1),\n",
       " (206, 307, 0.1),\n",
       " (39, 308, 0.2),\n",
       " (307, 206, 0.1),\n",
       " (1, 180, 0.1),\n",
       " (180, 1, 0.1),\n",
       " (235, 20, 0.1),\n",
       " (29, 198, 0.1),\n",
       " (198, 29, 0.1),\n",
       " (198, 60, 0.2),\n",
       " (29, 274, 0.2),\n",
       " (280, 29, 0.1),\n",
       " (274, 29, 0.2),\n",
       " (29, 280, 0.1),\n",
       " (202, 206, 0.3),\n",
       " (206, 202, 0.3),\n",
       " (274, 204, 0.1),\n",
       " (204, 274, 0.1),\n",
       " (204, 186, 0.2),\n",
       " (186, 204, 0.2),\n",
       " (7, 266, 0.2),\n",
       " (60, 223, 0.7),\n",
       " (257, 279, 0.4),\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "edges=[(data_matrix[\"From\"][i],data_matrix[\"To\"][i],data_matrix[\"Rating\"][i]/10) for i in data_matrix.index]\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in edges:\n",
    "    G.add_edge(edge[0],edge[1],weight=edge[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35592\n",
      "5881\n"
     ]
    }
   ],
   "source": [
    "print(len(G.edges()))\n",
    "print(len(G.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ht8rvQCzsSB"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jI-B_u0k6wB"
   },
   "outputs": [],
   "source": [
    "def initialize_scores(G):\n",
    "    fairness = {}\n",
    "    goodness = {}\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "    for node in nodes:\n",
    "        fairness[node] = 1\n",
    "        try:\n",
    "            goodness[node] = G.in_degree(node, weight='weight')*1.0/G.in_degree(node)\n",
    "        except:\n",
    "            goodness[node] = 0\n",
    "    return fairness, goodness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH-XbyIGpF99"
   },
   "outputs": [],
   "source": [
    "def Fariness_Goodness(G):\n",
    "    fairness, goodness = initialize_scores(G)\n",
    "    \n",
    "    nodes = G.nodes()\n",
    "\n",
    "    while True:\n",
    "        df = 0\n",
    "        dg = 0\n",
    "\n",
    "        \n",
    "        for node in tqdm(nodes):\n",
    "            inedges = G.in_edges(node, data='weight')\n",
    "            g = 0\n",
    "            for edge in inedges:\n",
    "                g += fairness[edge[0]]*edge[2]\n",
    "\n",
    "            try:\n",
    "                dg += abs(g/len(inedges) - goodness[node])\n",
    "                goodness[node] = g/len(inedges)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for node in nodes:\n",
    "            outedges = G.out_edges(node, data='weight')\n",
    "            f = 0\n",
    "            for edge in outedges:\n",
    "                f += 1.0 - abs(edge[2] - goodness[edge[1]])/2.0\n",
    "            try:\n",
    "                df += abs(f/len(outedges) - fairness[node])\n",
    "                fairness[node] = f/len(outedges)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if df < math.pow(10, -6) and dg < math.pow(10, -6):\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return fairness, goodness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hcjb6Bxdpie-",
    "outputId": "d07be4e5-d1c8-4c04-8909-e668996640c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674338/3518208791.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for node in tqdm(nodes):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8475d3168efe44e78411b0bffb96d550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dc2f46bf6541de8cea0170a40a5ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a61caa84e94151a15c892a933c2aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187449d9f8c14ef6bf85eafb918e4e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6002917734a045b781a87aee227b62f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a60e227c7524078a76de4294e26e9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f143826fe7408494c2410230ea86a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790d986edb90460fb0265a1679044c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8413746b40a4f52b98c8d33beb49781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01df16292c9439d92ce51b874fa7f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3940be73e54bb5a6b0f746a2162a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429c0091febb4153815e4a580caa839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df55f4f1be9e438fac66b30aa63b52bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b8545827384438af307dd1056b9007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152642bf459f406f9144b82bc3de5f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# these two dictionaries have the required scores\n",
    "fairness, goodness = Fariness_Goodness(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending Results to Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness to Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_mZdqNyMGXO"
   },
   "outputs": [],
   "source": [
    "def Fairness(row): return fairness[row]\n",
    "df=data_matrix.copy()\n",
    "df['fairness'] = df['From'].apply(Fairness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness to Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSfmx4XXNTOS"
   },
   "outputs": [],
   "source": [
    "def Goodness(row): return goodness[row]\n",
    "df['goodness'] = df['To'].apply(Goodness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "d051BQrrNcGj",
    "outputId": "7e72c573-52b3-4e3d-8086-95dcbf7d6102"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>fairness</th>\n",
       "      <th>goodness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.289242e+09</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.269531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.289242e+09</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.214168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.289243e+09</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.144465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.289245e+09</td>\n",
       "      <td>0.891287</td>\n",
       "      <td>-0.035796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1.289254e+09</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.756450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   From  To  Rating     Timestamp  fairness  goodness\n",
       "0     6   2       4  1.289242e+09  0.895726  0.269531\n",
       "1     6   5       2  1.289242e+09  0.895726  0.214168\n",
       "2     1  15       1  1.289243e+09  0.922436  0.144465\n",
       "3     4   3       7  1.289245e+09  0.891287 -0.035796\n",
       "4    13  16       8  1.289254e+09  0.945563  0.756450"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def betweenness_centrality(G):\n",
    "    \n",
    "    V=G.nodes()\n",
    "    pbar=tqdm(total=len(V))\n",
    "    btw={}\n",
    "    for s in V:\n",
    "        btw[s]=0.0\n",
    "    for s  in V:\n",
    "        pbar.update(1)\n",
    "        #single-source shortest paths problem\n",
    "        #initialization\n",
    "        pred={}\n",
    "        dist={}\n",
    "        sigma={}\n",
    "        Q=[]\n",
    "        stack=[]\n",
    "        for w in V:\n",
    "            pred[w]=[]\n",
    "            dist[w]=-1\n",
    "            sigma[w]=0\n",
    "            \n",
    "        dist[s]=0\n",
    "        sigma[s]=1\n",
    "        Q.append(s)\n",
    "        \n",
    "        while len(Q)!=0:\n",
    "            v=Q.pop(0)\n",
    "            stack.append(v)\n",
    "            for w in G.neighbors(v):\n",
    "                #path discovery\n",
    "                if dist[w]==-1:\n",
    "                    dist[w]=dist[v]+1\n",
    "                    \n",
    "                    Q.append(w)\n",
    "                #path counting\n",
    "                if dist[w]==dist[v]+1:\n",
    "                    sigma[w]=sigma[w]+sigma[v]\n",
    "                    dist[w]=dist[v]+1\n",
    "                    \n",
    "                    pred[w].append(v)\n",
    "        #accumulation\n",
    "        delta={}\n",
    "        for v in V:\n",
    "            delta[v]=0\n",
    "        while len(stack)!=0:\n",
    "            w=stack.pop()\n",
    "            for v in pred[w]:\n",
    "                delta[v]=delta[v]+(sigma[v]/sigma[w])*(1+delta[w])\n",
    "            if w !=s:\n",
    "                btw[w]=btw[w]+delta[w]\n",
    "        \n",
    "    return btw\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = G.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21492"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nx.Graph(G).edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674338/4108008526.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  pbar=tqdm(total=len(V))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02573424a47425b8880e962725eb7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "btw_centrality=betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value=max(btw_centrality.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Betweenness to Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674338/2035661013.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"btw\"][i]=((btw_centrality[(df[\"From\"][i])]+btw_centrality[(df[\"To\"][i])])/2)/max_value\n",
      "/tmp/ipykernel_674338/2035661013.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"btw_From\"][i]=btw_centrality[(df[\"From\"][i])]/max_value\n",
      "/tmp/ipykernel_674338/2035661013.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"btw_To\"][i]=btw_centrality[(df[\"To\"][i])]/max_value\n"
     ]
    }
   ],
   "source": [
    "df[\"btw_From\"]=0.\n",
    "df[\"btw_To\"]=0.\n",
    "df[\"btw\"]=0.\n",
    "\n",
    "for i in df.index:\n",
    "    df[\"btw\"][i]=((btw_centrality[(df[\"From\"][i])]+btw_centrality[(df[\"To\"][i])])/2)/max_value\n",
    "    df[\"btw_From\"][i]=btw_centrality[(df[\"From\"][i])]/max_value\n",
    "    df[\"btw_To\"][i]=btw_centrality[(df[\"To\"][i])]/max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "closeenness=nx.algorithms.closeness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674338/2958500815.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"C\"][i]=((closeenness[(df[\"From\"][i])]+closeenness[(df[\"To\"][i])])/2)\n",
      "/tmp/ipykernel_674338/2958500815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"C_From\"][i]=closeenness[(df[\"From\"][i])]\n",
      "/tmp/ipykernel_674338/2958500815.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"C_To\"][i]=closeenness[(df[\"To\"][i])]\n"
     ]
    }
   ],
   "source": [
    "df[\"C_From\"]=0.\n",
    "df[\"C_To\"]=0.\n",
    "df[\"C\"]=0.\n",
    "\n",
    "for i in df.index:\n",
    "    df[\"C\"][i]=((closeenness[(df[\"From\"][i])]+closeenness[(df[\"To\"][i])])/2)\n",
    "    df[\"C_From\"][i]=closeenness[(df[\"From\"][i])]\n",
    "    df[\"C_To\"][i]=closeenness[(df[\"To\"][i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_674338/3483378970.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"DC\"][i]=((degree_centrality[(df[\"From\"][i])]+degree_centrality[(df[\"To\"][i])])/2)\n",
      "/tmp/ipykernel_674338/3483378970.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"DC_From\"][i]=degree_centrality[(df[\"From\"][i])]\n",
      "/tmp/ipykernel_674338/3483378970.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"DC_To\"][i]=degree_centrality[(df[\"To\"][i])]\n"
     ]
    }
   ],
   "source": [
    "df[\"DC_From\"]=0.\n",
    "df[\"DC_To\"]=0.\n",
    "df[\"DC\"]=0.\n",
    "degree_centrality=nx.degree_centrality(G)\n",
    "\n",
    "for i in df.index:\n",
    "    df[\"DC\"][i]=((degree_centrality[(df[\"From\"][i])]+degree_centrality[(df[\"To\"][i])])/2)\n",
    "    df[\"DC_From\"][i]=degree_centrality[(df[\"From\"][i])]\n",
    "    df[\"DC_To\"][i]=degree_centrality[(df[\"To\"][i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acTXASLWNltZ"
   },
   "source": [
    "# Applying SL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1ulPVJWN97S"
   },
   "source": [
    "## Creating X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>fairness</th>\n",
       "      <th>goodness</th>\n",
       "      <th>btw_From</th>\n",
       "      <th>btw_To</th>\n",
       "      <th>btw</th>\n",
       "      <th>C_From</th>\n",
       "      <th>C_To</th>\n",
       "      <th>C</th>\n",
       "      <th>DC_From</th>\n",
       "      <th>DC_To</th>\n",
       "      <th>DC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.289242e+09</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.265506</td>\n",
       "      <td>0.280785</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014626</td>\n",
       "      <td>0.014456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.289242e+09</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009015</td>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.269230</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.289243e+09</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.144465</td>\n",
       "      <td>0.316636</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.160715</td>\n",
       "      <td>0.330994</td>\n",
       "      <td>0.251884</td>\n",
       "      <td>0.291439</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.039881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.289245e+09</td>\n",
       "      <td>0.891287</td>\n",
       "      <td>-0.035796</td>\n",
       "      <td>0.045986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.278002</td>\n",
       "      <td>0.262725</td>\n",
       "      <td>0.270364</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.011735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1.289254e+09</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.756450</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113416</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.229434</td>\n",
       "      <td>0.275087</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.034184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   From  To  Rating     Timestamp  fairness  goodness  btw_From    btw_To  \\\n",
       "0     6   2       4  1.289242e+09  0.895726  0.269531  0.018029  0.014119   \n",
       "1     6   5       2  1.289242e+09  0.895726  0.214168  0.018029  0.000000   \n",
       "2     1  15       1  1.289243e+09  0.922436  0.144465  0.316636  0.004795   \n",
       "3     4   3       7  1.289245e+09  0.891287 -0.035796  0.045986  0.000000   \n",
       "4    13  16       8  1.289254e+09  0.945563  0.756450  0.226833  0.000000   \n",
       "\n",
       "        btw    C_From      C_To         C   DC_From     DC_To        DC  \n",
       "0  0.016074  0.296063  0.265506  0.280785  0.014286  0.014626  0.014456  \n",
       "1  0.009015  0.296063  0.242396  0.269230  0.014286  0.001020  0.007653  \n",
       "2  0.160715  0.330994  0.251884  0.291439  0.075000  0.004762  0.039881  \n",
       "3  0.022993  0.278002  0.262725  0.270364  0.019898  0.003571  0.011735  \n",
       "4  0.113416  0.320739  0.229434  0.275087  0.068197  0.000170  0.034184  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Rating</th>\n",
       "      <th>fairness</th>\n",
       "      <th>goodness</th>\n",
       "      <th>btw</th>\n",
       "      <th>btw_From</th>\n",
       "      <th>btw_To</th>\n",
       "      <th>C_From</th>\n",
       "      <th>C_To</th>\n",
       "      <th>C</th>\n",
       "      <th>DC_From</th>\n",
       "      <th>DC_To</th>\n",
       "      <th>DC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.265506</td>\n",
       "      <td>0.280785</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014626</td>\n",
       "      <td>0.014456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>0.009015</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.269230</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.144465</td>\n",
       "      <td>0.160715</td>\n",
       "      <td>0.316636</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.330994</td>\n",
       "      <td>0.251884</td>\n",
       "      <td>0.291439</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.039881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.891287</td>\n",
       "      <td>-0.035796</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.045986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278002</td>\n",
       "      <td>0.262725</td>\n",
       "      <td>0.270364</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.011735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.756450</td>\n",
       "      <td>0.113416</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.229434</td>\n",
       "      <td>0.275087</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.034184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   From  To  Rating  fairness  goodness       btw  btw_From    btw_To  \\\n",
       "0     6   2       4  0.895726  0.269531  0.016074  0.018029  0.014119   \n",
       "1     6   5       2  0.895726  0.214168  0.009015  0.018029  0.000000   \n",
       "2     1  15       1  0.922436  0.144465  0.160715  0.316636  0.004795   \n",
       "3     4   3       7  0.891287 -0.035796  0.022993  0.045986  0.000000   \n",
       "4    13  16       8  0.945563  0.756450  0.113416  0.226833  0.000000   \n",
       "\n",
       "     C_From      C_To         C   DC_From     DC_To        DC  \n",
       "0  0.296063  0.265506  0.280785  0.014286  0.014626  0.014456  \n",
       "1  0.296063  0.242396  0.269230  0.014286  0.001020  0.007653  \n",
       "2  0.330994  0.251884  0.291439  0.075000  0.004762  0.039881  \n",
       "3  0.278002  0.262725  0.270364  0.019898  0.003571  0.011735  \n",
       "4  0.320739  0.229434  0.275087  0.068197  0.000170  0.034184  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['From', 'To', 'Rating', 'fairness', 'goodness', 'btw',\n",
    "       'btw_From', 'btw_To', 'C_From', 'C_To', 'C', 'DC_From', 'DC_To', 'DC']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=np.array(df[['fairness', 'goodness', 'btw',\n",
    "       'btw_From', 'btw_To', 'C_From', 'C_To', 'C', 'DC_From', 'DC_To', 'DC']])\n",
    "y=np.array(df[\"Rating\"])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.30,random_state = 20)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size = 0.3,random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RuYGzGR8wxGD"
   },
   "source": [
    "## Splitting data into test , train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pjh67yihOCcg"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.30,random_state = 20)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size = 0.3,random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss={\n",
    "    \n",
    "    1:[\"From\",\"To\"],\n",
    "    2: ['fairness', 'goodness', 'btw',\n",
    "       'btw_From', 'btw_To', 'C_From', 'C_To', 'C', 'DC_From', 'DC_To', 'DC'],\n",
    "    3:['fairness', 'goodness'],\n",
    "    4:['fairness', 'goodness', 'btw',\n",
    "       'btw_From', 'btw_To'],\n",
    "    5:['fairness', 'goodness', 'C_From', 'C_To', 'C', 'DC_From', 'DC_To', 'DC'],\n",
    "    6:[\n",
    "       'btw_From', 'btw_To', 'C_From', 'C_To', 'C', 'DC_From', 'DC_To', 'DC'],\n",
    "    7: [ 'DC_From', 'DC_To', 'DC']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 08:31:18.868787: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-19 08:31:18.868813: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "TRAIN_DATA_PATH = '/content/sample_data/california_housing_train.csv'\n",
    "TEST_DATA_PATH = '/content/sample_data/california_housing_test.csv'\n",
    "TARGET_NAME = 'median_house_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>fairness</th>\n",
       "      <th>goodness</th>\n",
       "      <th>btw_From</th>\n",
       "      <th>btw_To</th>\n",
       "      <th>btw</th>\n",
       "      <th>C_From</th>\n",
       "      <th>C_To</th>\n",
       "      <th>C</th>\n",
       "      <th>DC_From</th>\n",
       "      <th>DC_To</th>\n",
       "      <th>DC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.289242e+09</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.265506</td>\n",
       "      <td>0.280785</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014626</td>\n",
       "      <td>0.014456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.289242e+09</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009015</td>\n",
       "      <td>0.296063</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.269230</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.289243e+09</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.144465</td>\n",
       "      <td>0.316636</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.160715</td>\n",
       "      <td>0.330994</td>\n",
       "      <td>0.251884</td>\n",
       "      <td>0.291439</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.039881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.289245e+09</td>\n",
       "      <td>0.891287</td>\n",
       "      <td>-0.035796</td>\n",
       "      <td>0.045986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.278002</td>\n",
       "      <td>0.262725</td>\n",
       "      <td>0.270364</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.011735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1.289254e+09</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.756450</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113416</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.229434</td>\n",
       "      <td>0.275087</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.034184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>4559</td>\n",
       "      <td>5205</td>\n",
       "      <td>-10</td>\n",
       "      <td>1.387183e+09</td>\n",
       "      <td>0.922255</td>\n",
       "      <td>-0.921246</td>\n",
       "      <td>0.048925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>0.290045</td>\n",
       "      <td>0.237379</td>\n",
       "      <td>0.263712</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.016327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>2045</td>\n",
       "      <td>5205</td>\n",
       "      <td>-10</td>\n",
       "      <td>1.387183e+09</td>\n",
       "      <td>0.909661</td>\n",
       "      <td>-0.921246</td>\n",
       "      <td>0.096884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048442</td>\n",
       "      <td>0.302241</td>\n",
       "      <td>0.237379</td>\n",
       "      <td>0.269810</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.025595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>2045</td>\n",
       "      <td>5197</td>\n",
       "      <td>-10</td>\n",
       "      <td>1.387184e+09</td>\n",
       "      <td>0.909661</td>\n",
       "      <td>-0.654554</td>\n",
       "      <td>0.096884</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.048871</td>\n",
       "      <td>0.302241</td>\n",
       "      <td>0.238258</td>\n",
       "      <td>0.270249</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.026786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>2045</td>\n",
       "      <td>5198</td>\n",
       "      <td>-10</td>\n",
       "      <td>1.387184e+09</td>\n",
       "      <td>0.909661</td>\n",
       "      <td>-0.615708</td>\n",
       "      <td>0.096884</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.048842</td>\n",
       "      <td>0.302241</td>\n",
       "      <td>0.237782</td>\n",
       "      <td>0.270011</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.026616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>4559</td>\n",
       "      <td>5206</td>\n",
       "      <td>1</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.922255</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.048925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>0.290045</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.264284</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.016327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30001 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       From    To  Rating     Timestamp  fairness  goodness  btw_From  \\\n",
       "0         6     2       4  1.289242e+09  0.895726  0.269531  0.018029   \n",
       "1         6     5       2  1.289242e+09  0.895726  0.214168  0.018029   \n",
       "2         1    15       1  1.289243e+09  0.922436  0.144465  0.316636   \n",
       "3         4     3       7  1.289245e+09  0.891287 -0.035796  0.045986   \n",
       "4        13    16       8  1.289254e+09  0.945563  0.756450  0.226833   \n",
       "...     ...   ...     ...           ...       ...       ...       ...   \n",
       "29996  4559  5205     -10  1.387183e+09  0.922255 -0.921246  0.048925   \n",
       "29997  2045  5205     -10  1.387183e+09  0.909661 -0.921246  0.096884   \n",
       "29998  2045  5197     -10  1.387184e+09  0.909661 -0.654554  0.096884   \n",
       "29999  2045  5198     -10  1.387184e+09  0.909661 -0.615708  0.096884   \n",
       "30000  4559  5206       1  1.387188e+09  0.922255 -0.243809  0.048925   \n",
       "\n",
       "         btw_To       btw    C_From      C_To         C   DC_From     DC_To  \\\n",
       "0      0.014119  0.016074  0.296063  0.265506  0.280785  0.014286  0.014626   \n",
       "1      0.000000  0.009015  0.296063  0.242396  0.269230  0.014286  0.001020   \n",
       "2      0.004795  0.160715  0.330994  0.251884  0.291439  0.075000  0.004762   \n",
       "3      0.000000  0.022993  0.278002  0.262725  0.270364  0.019898  0.003571   \n",
       "4      0.000000  0.113416  0.320739  0.229434  0.275087  0.068197  0.000170   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "29996  0.000000  0.024462  0.290045  0.237379  0.263712  0.032143  0.000510   \n",
       "29997  0.000000  0.048442  0.302241  0.237379  0.269810  0.050680  0.000510   \n",
       "29998  0.000858  0.048871  0.302241  0.238258  0.270249  0.050680  0.002891   \n",
       "29999  0.000800  0.048842  0.302241  0.237782  0.270011  0.050680  0.002551   \n",
       "30000  0.000000  0.024462  0.290045  0.238523  0.264284  0.032143  0.000510   \n",
       "\n",
       "             DC  \n",
       "0      0.014456  \n",
       "1      0.007653  \n",
       "2      0.039881  \n",
       "3      0.011735  \n",
       "4      0.034184  \n",
       "...         ...  \n",
       "29996  0.016327  \n",
       "29997  0.025595  \n",
       "29998  0.026786  \n",
       "29999  0.026616  \n",
       "30000  0.016327  \n",
       "\n",
       "[30001 rows x 15 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = features, y_train = ratings\n",
    "x_train, y_train = df.loc[:30000].drop(columns=[\"Rating\"]),df.loc[:30000][\"Rating\"]\n",
    "x_test, y_test = df.loc[30000:].drop(columns=[\"Rating\"]),df.loc[30000:][\"Rating\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzUj9vgRyjef"
   },
   "source": [
    "## Using NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_datasets(x_train, x_test):\n",
    "\n",
    "  \"\"\"\n",
    "  Standard Scale test and train data\n",
    "  Z - Score normalization\n",
    "  \"\"\"\n",
    "  standard_scaler = StandardScaler()\n",
    "  x_train_scaled = pd.DataFrame(\n",
    "      standard_scaler.fit_transform(x_train),\n",
    "      columns=x_train.columns\n",
    "  )\n",
    "  x_test_scaled = pd.DataFrame(\n",
    "      standard_scaler.transform(x_test),\n",
    "      columns = x_test.columns\n",
    "  )\n",
    "  return x_train_scaled, x_test_scaled\n",
    "x_train_scaled, x_test_scaled = scale_datasets(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units1 = 160\n",
    "hidden_units2 = 480\n",
    "hidden_units3 = 256\n",
    "learning_rate = 0.001\n",
    "# Creating model using the Sequential in tensorflow\n",
    "def build_model_using_sequential():\n",
    "  model = Sequential([\n",
    "    Dense(hidden_units1, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(hidden_units2, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(hidden_units3, kernel_initializer='normal', activation='relu'),\n",
    "    Dense(1, kernel_initializer='normal', activation='linear')\n",
    "  ])\n",
    "  return model\n",
    "# build the model\n",
    "model = build_model_using_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.2048 - mean_squared_logarithmic_error: 0.2045 - val_loss: 0.2570 - val_mean_squared_logarithmic_error: 0.2597\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1623 - mean_squared_logarithmic_error: 0.1622 - val_loss: 0.2434 - val_mean_squared_logarithmic_error: 0.2457\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1596 - mean_squared_logarithmic_error: 0.1597 - val_loss: 0.2630 - val_mean_squared_logarithmic_error: 0.2656\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_logarithmic_error: 0.1559 - val_loss: 0.2426 - val_mean_squared_logarithmic_error: 0.2450\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1541 - mean_squared_logarithmic_error: 0.1541 - val_loss: 0.2223 - val_mean_squared_logarithmic_error: 0.2240\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1523 - mean_squared_logarithmic_error: 0.1522 - val_loss: 0.3119 - val_mean_squared_logarithmic_error: 0.3149\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1504 - mean_squared_logarithmic_error: 0.1503 - val_loss: 0.3185 - val_mean_squared_logarithmic_error: 0.3217\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1495 - mean_squared_logarithmic_error: 0.1493 - val_loss: 0.3010 - val_mean_squared_logarithmic_error: 0.3037\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1465 - mean_squared_logarithmic_error: 0.1464 - val_loss: 0.3281 - val_mean_squared_logarithmic_error: 0.3312\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 1s 8ms/step - loss: 0.1473 - mean_squared_logarithmic_error: 0.1474 - val_loss: 0.2897 - val_mean_squared_logarithmic_error: 0.2925\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=learning_rate), \n",
    "    metrics=[msle]\n",
    ")\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    x_train_scaled.values, \n",
    "    y_train.values, \n",
    "    epochs=10, \n",
    "    batch_size=128,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history, key):\n",
    "  plt.plot(history.history[key])\n",
    "  plt.plot(history.history['val_'+key])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(key)\n",
    "  plt.legend([key, 'val_'+key])\n",
    "  plt.show()\n",
    "# Plot the history\n",
    "plot_history(history, 'mean_squared_logarithmic_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['prediction'] = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>fairness</th>\n",
       "      <th>goodness</th>\n",
       "      <th>btw_From</th>\n",
       "      <th>btw_To</th>\n",
       "      <th>btw</th>\n",
       "      <th>C_From</th>\n",
       "      <th>C_To</th>\n",
       "      <th>C</th>\n",
       "      <th>DC_From</th>\n",
       "      <th>DC_To</th>\n",
       "      <th>DC</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>4559</td>\n",
       "      <td>5206</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.922255</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.048925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>0.290045</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.264284</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.839857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>4172</td>\n",
       "      <td>5206</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.860079</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.294827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147414</td>\n",
       "      <td>0.313895</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.276209</td>\n",
       "      <td>0.082653</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.812081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>2045</td>\n",
       "      <td>5206</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.909661</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.096884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048442</td>\n",
       "      <td>0.302241</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.270382</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.628481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>5186</td>\n",
       "      <td>5193</td>\n",
       "      <td>1.387206e+09</td>\n",
       "      <td>0.881214</td>\n",
       "      <td>-0.679075</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.217999</td>\n",
       "      <td>0.238586</td>\n",
       "      <td>0.228292</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.312962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>2125</td>\n",
       "      <td>5193</td>\n",
       "      <td>1.387207e+09</td>\n",
       "      <td>0.931823</td>\n",
       "      <td>-0.679075</td>\n",
       "      <td>0.292961</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.147380</td>\n",
       "      <td>0.309157</td>\n",
       "      <td>0.238586</td>\n",
       "      <td>0.273872</td>\n",
       "      <td>0.098129</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.050765</td>\n",
       "      <td>0.098959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35587</th>\n",
       "      <td>4499</td>\n",
       "      <td>1810</td>\n",
       "      <td>1.453612e+09</td>\n",
       "      <td>0.924509</td>\n",
       "      <td>0.103462</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>0.348557</td>\n",
       "      <td>0.180302</td>\n",
       "      <td>0.283379</td>\n",
       "      <td>0.317161</td>\n",
       "      <td>0.300270</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.121599</td>\n",
       "      <td>0.067347</td>\n",
       "      <td>2.789482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35588</th>\n",
       "      <td>2731</td>\n",
       "      <td>3901</td>\n",
       "      <td>1.453679e+09</td>\n",
       "      <td>0.902977</td>\n",
       "      <td>0.170051</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.244937</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.239944</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>2.532463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35589</th>\n",
       "      <td>2731</td>\n",
       "      <td>4897</td>\n",
       "      <td>1.453679e+09</td>\n",
       "      <td>0.902977</td>\n",
       "      <td>0.175835</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.244937</td>\n",
       "      <td>0.202711</td>\n",
       "      <td>0.223824</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>2.249707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35590</th>\n",
       "      <td>13</td>\n",
       "      <td>1128</td>\n",
       "      <td>1.453680e+09</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.123803</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.238332</td>\n",
       "      <td>0.279536</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.035289</td>\n",
       "      <td>1.559875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35591</th>\n",
       "      <td>1128</td>\n",
       "      <td>13</td>\n",
       "      <td>1.453684e+09</td>\n",
       "      <td>0.983498</td>\n",
       "      <td>0.167373</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.238332</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.279536</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.035289</td>\n",
       "      <td>3.167675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5592 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       From    To     Timestamp  fairness  goodness  btw_From    btw_To  \\\n",
       "30000  4559  5206  1.387188e+09  0.922255 -0.243809  0.048925  0.000000   \n",
       "30001  4172  5206  1.387188e+09  0.860079 -0.243809  0.294827  0.000000   \n",
       "30002  2045  5206  1.387188e+09  0.909661 -0.243809  0.096884  0.000000   \n",
       "30003  5186  5193  1.387206e+09  0.881214 -0.679075  0.001422  0.001799   \n",
       "30004  2125  5193  1.387207e+09  0.931823 -0.679075  0.292961  0.001799   \n",
       "...     ...   ...           ...       ...       ...       ...       ...   \n",
       "35587  4499  1810  1.453612e+09  0.924509  0.103462  0.012046  0.348557   \n",
       "35588  2731  3901  1.453679e+09  0.902977  0.170051  0.001214  0.000214   \n",
       "35589  2731  4897  1.453679e+09  0.902977  0.175835  0.001214  0.001322   \n",
       "35590    13  1128  1.453680e+09  0.945563  0.123803  0.226833  0.000689   \n",
       "35591  1128    13  1.453684e+09  0.983498  0.167373  0.000689  0.226833   \n",
       "\n",
       "            btw    C_From      C_To         C   DC_From     DC_To        DC  \\\n",
       "30000  0.024462  0.290045  0.238523  0.264284  0.032143  0.000510  0.016327   \n",
       "30001  0.147414  0.313895  0.238523  0.276209  0.082653  0.000510  0.041582   \n",
       "30002  0.048442  0.302241  0.238523  0.270382  0.050680  0.000510  0.025595   \n",
       "30003  0.001611  0.217999  0.238586  0.228292  0.001871  0.003401  0.002636   \n",
       "30004  0.147380  0.309157  0.238586  0.273872  0.098129  0.003401  0.050765   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "35587  0.180302  0.283379  0.317161  0.300270  0.013095  0.121599  0.067347   \n",
       "35588  0.000714  0.244937  0.234952  0.239944  0.002381  0.002041  0.002211   \n",
       "35589  0.001268  0.244937  0.202711  0.223824  0.002381  0.001361  0.001871   \n",
       "35590  0.113761  0.320739  0.238332  0.279536  0.068197  0.002381  0.035289   \n",
       "35591  0.113761  0.238332  0.320739  0.279536  0.002381  0.068197  0.035289   \n",
       "\n",
       "       prediction  \n",
       "30000    0.839857  \n",
       "30001    0.812081  \n",
       "30002    0.628481  \n",
       "30003    0.312962  \n",
       "30004    0.098959  \n",
       "...           ...  \n",
       "35587    2.789482  \n",
       "35588    2.532463  \n",
       "35589    2.249707  \n",
       "35590    1.559875  \n",
       "35591    3.167675  \n",
       "\n",
       "[5592 rows x 15 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>fairness</th>\n",
       "      <th>goodness</th>\n",
       "      <th>btw_From</th>\n",
       "      <th>btw_To</th>\n",
       "      <th>btw</th>\n",
       "      <th>C_From</th>\n",
       "      <th>C_To</th>\n",
       "      <th>C</th>\n",
       "      <th>DC_From</th>\n",
       "      <th>DC_To</th>\n",
       "      <th>DC</th>\n",
       "      <th>prediction</th>\n",
       "      <th>NN1_DIFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>4559</td>\n",
       "      <td>5206</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.922255</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.048925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>0.290045</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.264284</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.839857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>4172</td>\n",
       "      <td>5206</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.860079</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.294827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147414</td>\n",
       "      <td>0.313895</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.276209</td>\n",
       "      <td>0.082653</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.812081</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>2045</td>\n",
       "      <td>5206</td>\n",
       "      <td>1.387188e+09</td>\n",
       "      <td>0.909661</td>\n",
       "      <td>-0.243809</td>\n",
       "      <td>0.096884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048442</td>\n",
       "      <td>0.302241</td>\n",
       "      <td>0.238523</td>\n",
       "      <td>0.270382</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.628481</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>5186</td>\n",
       "      <td>5193</td>\n",
       "      <td>1.387206e+09</td>\n",
       "      <td>0.881214</td>\n",
       "      <td>-0.679075</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.217999</td>\n",
       "      <td>0.238586</td>\n",
       "      <td>0.228292</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.312962</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>2125</td>\n",
       "      <td>5193</td>\n",
       "      <td>1.387207e+09</td>\n",
       "      <td>0.931823</td>\n",
       "      <td>-0.679075</td>\n",
       "      <td>0.292961</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.147380</td>\n",
       "      <td>0.309157</td>\n",
       "      <td>0.238586</td>\n",
       "      <td>0.273872</td>\n",
       "      <td>0.098129</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.050765</td>\n",
       "      <td>0.098959</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35587</th>\n",
       "      <td>4499</td>\n",
       "      <td>1810</td>\n",
       "      <td>1.453612e+09</td>\n",
       "      <td>0.924509</td>\n",
       "      <td>0.103462</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>0.348557</td>\n",
       "      <td>0.180302</td>\n",
       "      <td>0.283379</td>\n",
       "      <td>0.317161</td>\n",
       "      <td>0.300270</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.121599</td>\n",
       "      <td>0.067347</td>\n",
       "      <td>2.789482</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35588</th>\n",
       "      <td>2731</td>\n",
       "      <td>3901</td>\n",
       "      <td>1.453679e+09</td>\n",
       "      <td>0.902977</td>\n",
       "      <td>0.170051</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.244937</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.239944</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>2.532463</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35589</th>\n",
       "      <td>2731</td>\n",
       "      <td>4897</td>\n",
       "      <td>1.453679e+09</td>\n",
       "      <td>0.902977</td>\n",
       "      <td>0.175835</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.244937</td>\n",
       "      <td>0.202711</td>\n",
       "      <td>0.223824</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>2.249707</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35590</th>\n",
       "      <td>13</td>\n",
       "      <td>1128</td>\n",
       "      <td>1.453680e+09</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.123803</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.238332</td>\n",
       "      <td>0.279536</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.035289</td>\n",
       "      <td>1.559875</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35591</th>\n",
       "      <td>1128</td>\n",
       "      <td>13</td>\n",
       "      <td>1.453684e+09</td>\n",
       "      <td>0.983498</td>\n",
       "      <td>0.167373</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.226833</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>0.238332</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.279536</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.068197</td>\n",
       "      <td>0.035289</td>\n",
       "      <td>3.167675</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5592 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       From    To     Timestamp  fairness  goodness  btw_From    btw_To  \\\n",
       "30000  4559  5206  1.387188e+09  0.922255 -0.243809  0.048925  0.000000   \n",
       "30001  4172  5206  1.387188e+09  0.860079 -0.243809  0.294827  0.000000   \n",
       "30002  2045  5206  1.387188e+09  0.909661 -0.243809  0.096884  0.000000   \n",
       "30003  5186  5193  1.387206e+09  0.881214 -0.679075  0.001422  0.001799   \n",
       "30004  2125  5193  1.387207e+09  0.931823 -0.679075  0.292961  0.001799   \n",
       "...     ...   ...           ...       ...       ...       ...       ...   \n",
       "35587  4499  1810  1.453612e+09  0.924509  0.103462  0.012046  0.348557   \n",
       "35588  2731  3901  1.453679e+09  0.902977  0.170051  0.001214  0.000214   \n",
       "35589  2731  4897  1.453679e+09  0.902977  0.175835  0.001214  0.001322   \n",
       "35590    13  1128  1.453680e+09  0.945563  0.123803  0.226833  0.000689   \n",
       "35591  1128    13  1.453684e+09  0.983498  0.167373  0.000689  0.226833   \n",
       "\n",
       "            btw    C_From      C_To         C   DC_From     DC_To        DC  \\\n",
       "30000  0.024462  0.290045  0.238523  0.264284  0.032143  0.000510  0.016327   \n",
       "30001  0.147414  0.313895  0.238523  0.276209  0.082653  0.000510  0.041582   \n",
       "30002  0.048442  0.302241  0.238523  0.270382  0.050680  0.000510  0.025595   \n",
       "30003  0.001611  0.217999  0.238586  0.228292  0.001871  0.003401  0.002636   \n",
       "30004  0.147380  0.309157  0.238586  0.273872  0.098129  0.003401  0.050765   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "35587  0.180302  0.283379  0.317161  0.300270  0.013095  0.121599  0.067347   \n",
       "35588  0.000714  0.244937  0.234952  0.239944  0.002381  0.002041  0.002211   \n",
       "35589  0.001268  0.244937  0.202711  0.223824  0.002381  0.001361  0.001871   \n",
       "35590  0.113761  0.320739  0.238332  0.279536  0.068197  0.002381  0.035289   \n",
       "35591  0.113761  0.238332  0.320739  0.279536  0.002381  0.068197  0.035289   \n",
       "\n",
       "       prediction  NN1_DIFF  \n",
       "30000    0.839857       0.0  \n",
       "30001    0.812081       0.0  \n",
       "30002    0.628481     -11.0  \n",
       "30003    0.312962     -10.0  \n",
       "30004    0.098959     -10.0  \n",
       "...           ...       ...  \n",
       "35587    2.789482      -2.0  \n",
       "35588    2.532463       2.0  \n",
       "35589    2.249707       3.0  \n",
       "35590    1.559875      -1.0  \n",
       "35591    3.167675      -1.0  \n",
       "\n",
       "[5592 rows x 16 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[\"NN1_DIFF\"]=y_test-round(x_test['prediction'])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based DNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Initial_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               7680      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 500,481\n",
      "Trainable params: 500,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = x_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(512, input_dim = x_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "],name=\"Initial_model\",)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 1.6990 - accuracy: 0.5736 - val_loss: 1.9084 - val_accuracy: 0.5079\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6984 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6972 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6954 - accuracy: 0.5736 - val_loss: 1.9055 - val_accuracy: 0.5079\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6946 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6941 - accuracy: 0.5736 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6945 - accuracy: 0.5736 - val_loss: 1.9057 - val_accuracy: 0.5079\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6998 - accuracy: 0.5736 - val_loss: 1.9031 - val_accuracy: 0.5079\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6977 - accuracy: 0.5736 - val_loss: 1.9021 - val_accuracy: 0.5079\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9042 - val_accuracy: 0.5079\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9024 - val_accuracy: 0.5079\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6938 - accuracy: 0.5736 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6969 - accuracy: 0.5736 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6996 - accuracy: 0.5736 - val_loss: 1.9042 - val_accuracy: 0.5079\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6987 - accuracy: 0.5736 - val_loss: 1.9068 - val_accuracy: 0.5079\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6982 - accuracy: 0.5736 - val_loss: 1.9056 - val_accuracy: 0.5079\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6968 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6950 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6961 - accuracy: 0.5736 - val_loss: 1.9062 - val_accuracy: 0.5079\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6994 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9019 - val_accuracy: 0.5079\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6973 - accuracy: 0.5736 - val_loss: 1.9036 - val_accuracy: 0.5079\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6953 - accuracy: 0.5736 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6967 - accuracy: 0.5736 - val_loss: 1.9028 - val_accuracy: 0.5079\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9014 - val_accuracy: 0.5079\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6940 - accuracy: 0.5736 - val_loss: 1.9013 - val_accuracy: 0.5079\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6939 - accuracy: 0.5736 - val_loss: 1.9058 - val_accuracy: 0.5079\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6953 - accuracy: 0.5736 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9073 - val_accuracy: 0.5079\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9033 - val_accuracy: 0.5079\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6974 - accuracy: 0.5736 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9055 - val_accuracy: 0.5079\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6949 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6964 - accuracy: 0.5736 - val_loss: 1.9014 - val_accuracy: 0.5079\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6985 - accuracy: 0.5736 - val_loss: 1.9037 - val_accuracy: 0.5079\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6982 - accuracy: 0.5736 - val_loss: 1.9086 - val_accuracy: 0.5079\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6959 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9028 - val_accuracy: 0.5079\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6949 - accuracy: 0.5736 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6950 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9013 - val_accuracy: 0.5079\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9028 - val_accuracy: 0.5079\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6956 - accuracy: 0.5736 - val_loss: 1.9080 - val_accuracy: 0.5079\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6968 - accuracy: 0.5736 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9048 - val_accuracy: 0.5079\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9027 - val_accuracy: 0.5079\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6959 - accuracy: 0.5736 - val_loss: 1.9051 - val_accuracy: 0.5079\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6954 - accuracy: 0.5736 - val_loss: 1.9029 - val_accuracy: 0.5079\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6946 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6956 - accuracy: 0.5736 - val_loss: 1.9065 - val_accuracy: 0.5079\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9049 - val_accuracy: 0.5079\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9063 - val_accuracy: 0.5079\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6997 - accuracy: 0.5736 - val_loss: 1.9081 - val_accuracy: 0.5079\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6949 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6950 - accuracy: 0.5736 - val_loss: 1.9013 - val_accuracy: 0.5079\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9068 - val_accuracy: 0.5079\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6975 - accuracy: 0.5736 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9046 - val_accuracy: 0.5079\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9017 - val_accuracy: 0.5079\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6964 - accuracy: 0.5736 - val_loss: 1.9021 - val_accuracy: 0.5079\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9045 - val_accuracy: 0.5079\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9033 - val_accuracy: 0.5079\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9017 - val_accuracy: 0.5079\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6954 - accuracy: 0.5736 - val_loss: 1.9066 - val_accuracy: 0.5079\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9031 - val_accuracy: 0.5079\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9017 - val_accuracy: 0.5079\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6953 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9043 - val_accuracy: 0.5079\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6970 - accuracy: 0.5736 - val_loss: 1.9020 - val_accuracy: 0.5079\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9037 - val_accuracy: 0.5079\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6945 - accuracy: 0.5736 - val_loss: 1.9035 - val_accuracy: 0.5079\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6971 - accuracy: 0.5736 - val_loss: 1.9022 - val_accuracy: 0.5079\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9049 - val_accuracy: 0.5079\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6956 - accuracy: 0.5736 - val_loss: 1.9042 - val_accuracy: 0.5079\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6957 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6949 - accuracy: 0.5736 - val_loss: 1.9019 - val_accuracy: 0.5079\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9032 - val_accuracy: 0.5079\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6967 - accuracy: 0.5736 - val_loss: 1.9091 - val_accuracy: 0.5079\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6969 - accuracy: 0.5736 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9027 - val_accuracy: 0.5079\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6980 - accuracy: 0.5736 - val_loss: 1.9083 - val_accuracy: 0.5079\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6975 - accuracy: 0.5736 - val_loss: 1.9026 - val_accuracy: 0.5079\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6959 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9030 - val_accuracy: 0.5079\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6945 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9031 - val_accuracy: 0.5079\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6936 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9031 - val_accuracy: 0.5079\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6959 - accuracy: 0.5736 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9021 - val_accuracy: 0.5079\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6955 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9036 - val_accuracy: 0.5079\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6959 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6954 - accuracy: 0.5736 - val_loss: 1.9071 - val_accuracy: 0.5079\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6950 - accuracy: 0.5736 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6951 - accuracy: 0.5736 - val_loss: 1.9041 - val_accuracy: 0.5079\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6967 - accuracy: 0.5736 - val_loss: 1.9031 - val_accuracy: 0.5079\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6961 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9022 - val_accuracy: 0.5079\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6950 - accuracy: 0.5736 - val_loss: 1.9034 - val_accuracy: 0.5079\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6949 - accuracy: 0.5736 - val_loss: 1.9040 - val_accuracy: 0.5079\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6945 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6935 - accuracy: 0.5736 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9053 - val_accuracy: 0.5079\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6991 - accuracy: 0.5736 - val_loss: 1.9028 - val_accuracy: 0.5079\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6935 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9021 - val_accuracy: 0.5079\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9014 - val_accuracy: 0.5079\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9055 - val_accuracy: 0.5079\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6949 - accuracy: 0.5736 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6935 - accuracy: 0.5736 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6948 - accuracy: 0.5736 - val_loss: 1.9056 - val_accuracy: 0.5079\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6955 - accuracy: 0.5736 - val_loss: 1.9012 - val_accuracy: 0.5079\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9026 - val_accuracy: 0.5079\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6950 - accuracy: 0.5736 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6939 - accuracy: 0.5736 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6941 - accuracy: 0.5736 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6946 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9049 - val_accuracy: 0.5079\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6955 - accuracy: 0.5736 - val_loss: 1.9060 - val_accuracy: 0.5079\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6959 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9048 - val_accuracy: 0.5079\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9034 - val_accuracy: 0.5079\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6955 - accuracy: 0.5736 - val_loss: 1.9058 - val_accuracy: 0.5079\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6954 - accuracy: 0.5736 - val_loss: 1.9022 - val_accuracy: 0.5079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6940 - accuracy: 0.5736 - val_loss: 1.9024 - val_accuracy: 0.5079\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9022 - val_accuracy: 0.5079\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6939 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9026 - val_accuracy: 0.5079\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6955 - accuracy: 0.5736 - val_loss: 1.9026 - val_accuracy: 0.5079\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9015 - val_accuracy: 0.5079\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6935 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6964 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6957 - accuracy: 0.5736 - val_loss: 1.9017 - val_accuracy: 0.5079\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6946 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6940 - accuracy: 0.5736 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6956 - accuracy: 0.5736 - val_loss: 1.9034 - val_accuracy: 0.5079\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9054 - val_accuracy: 0.5079\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6954 - accuracy: 0.5736 - val_loss: 1.9016 - val_accuracy: 0.5079\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6946 - accuracy: 0.5736 - val_loss: 1.9020 - val_accuracy: 0.5079\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6939 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6940 - accuracy: 0.5736 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6944 - accuracy: 0.5736 - val_loss: 1.9021 - val_accuracy: 0.5079\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 0s 17ms/step - loss: 1.6937 - accuracy: 0.5736 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9050 - val_accuracy: 0.5079\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 0s 16ms/step - loss: 1.6943 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6952 - accuracy: 0.5736 - val_loss: 1.9020 - val_accuracy: 0.5079\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6947 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 1.6953 - accuracy: 0.5736 - val_loss: 1.9049 - val_accuracy: 0.5079\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9037 - val_accuracy: 0.5079\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 1.6942 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n"
     ]
    }
   ],
   "source": [
    "import keras.optimizers\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='mean_absolute_error',metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(x_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Our first model turned out to be quite a failure, \n",
    "we have horrendous overfitting on Training data and there is a a gap\n",
    "betweeen Validation Loss and train loss for the entire epochs.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = x_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "],name=\"Dropout\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 10848780.0000 - accuracy: 0.2852 - val_loss: 351564.5312 - val_accuracy: 0.5079\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1261530.7500 - accuracy: 0.2807 - val_loss: 70716.6328 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 670962.8125 - accuracy: 0.2833 - val_loss: 9038.5146 - val_accuracy: 0.5079\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 394301.4688 - accuracy: 0.2849 - val_loss: 16049.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 182111.2188 - accuracy: 0.2626 - val_loss: 2.5859 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 50604.5195 - accuracy: 0.1069 - val_loss: 2.4862 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 20078.1602 - accuracy: 0.0493 - val_loss: 2.3497 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 12556.9111 - accuracy: 0.1802 - val_loss: 2.2011 - val_accuracy: 0.5079\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 8759.0420 - accuracy: 0.5498 - val_loss: 2.0653 - val_accuracy: 0.5079\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 6985.1143 - accuracy: 0.5571 - val_loss: 1.9432 - val_accuracy: 0.5079\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 5507.3828 - accuracy: 0.5575 - val_loss: 1.9090 - val_accuracy: 0.5079\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 4694.4219 - accuracy: 0.5611 - val_loss: 1.9022 - val_accuracy: 0.5079\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 4234.4727 - accuracy: 0.5625 - val_loss: 1.9029 - val_accuracy: 0.5079\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 3494.5894 - accuracy: 0.5636 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 3167.0876 - accuracy: 0.5644 - val_loss: 1.9019 - val_accuracy: 0.5079\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 3087.1479 - accuracy: 0.5659 - val_loss: 1.9017 - val_accuracy: 0.5079\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 2777.9824 - accuracy: 0.5664 - val_loss: 1.9014 - val_accuracy: 0.5079\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 2419.7805 - accuracy: 0.5660 - val_loss: 1.9017 - val_accuracy: 0.5079\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 2156.2354 - accuracy: 0.5676 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 1970.7915 - accuracy: 0.5681 - val_loss: 1.9013 - val_accuracy: 0.5079\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1987.7394 - accuracy: 0.5683 - val_loss: 1.9016 - val_accuracy: 0.5079\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1644.6204 - accuracy: 0.5694 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1515.6262 - accuracy: 0.5687 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 1467.5630 - accuracy: 0.5694 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1355.0333 - accuracy: 0.5694 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 1657.0579 - accuracy: 0.5696 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1417.4978 - accuracy: 0.5704 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1326.4454 - accuracy: 0.5709 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 1175.5339 - accuracy: 0.5704 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1055.6586 - accuracy: 0.5705 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1037.1029 - accuracy: 0.5707 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1204.7334 - accuracy: 0.5709 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1290.7642 - accuracy: 0.5707 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1097.5056 - accuracy: 0.5713 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 802.0728 - accuracy: 0.5713 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 950.7825 - accuracy: 0.5712 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 798.5767 - accuracy: 0.5717 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 793.6767 - accuracy: 0.5711 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 916.3646 - accuracy: 0.5715 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 736.6221 - accuracy: 0.5716 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 785.8583 - accuracy: 0.5715 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 743.2404 - accuracy: 0.5719 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 628.4877 - accuracy: 0.5720 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 820.3398 - accuracy: 0.5715 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 816.1368 - accuracy: 0.5719 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 743.4984 - accuracy: 0.5715 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 668.3352 - accuracy: 0.5722 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 736.1593 - accuracy: 0.5716 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 771.4261 - accuracy: 0.5719 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 693.7248 - accuracy: 0.5721 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 562.0657 - accuracy: 0.5723 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 612.5164 - accuracy: 0.5722 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 718.7932 - accuracy: 0.5722 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 583.8469 - accuracy: 0.5722 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 586.3717 - accuracy: 0.5720 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 561.1777 - accuracy: 0.5723 - val_loss: 1.9002 - val_accuracy: 0.5079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 476.5392 - accuracy: 0.5725 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 734.9467 - accuracy: 0.5719 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 467.8140 - accuracy: 0.5726 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 499.3810 - accuracy: 0.5721 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 521.1318 - accuracy: 0.5728 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 512.4377 - accuracy: 0.5728 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 600.3236 - accuracy: 0.5726 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 493.9399 - accuracy: 0.5726 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 455.7808 - accuracy: 0.5724 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 471.2949 - accuracy: 0.5725 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 437.3152 - accuracy: 0.5726 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 481.5189 - accuracy: 0.5721 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 399.5558 - accuracy: 0.5726 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 330.1805 - accuracy: 0.5726 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 405.3652 - accuracy: 0.5726 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 511.8172 - accuracy: 0.5725 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 356.4992 - accuracy: 0.5727 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 299.5120 - accuracy: 0.5728 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 300.8186 - accuracy: 0.5727 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 351.1506 - accuracy: 0.5727 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 454.2797 - accuracy: 0.5724 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 488.5422 - accuracy: 0.5726 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 327.3724 - accuracy: 0.5726 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 264.8655 - accuracy: 0.5729 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 355.5891 - accuracy: 0.5728 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 363.9750 - accuracy: 0.5727 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 449.8656 - accuracy: 0.5728 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 303.5782 - accuracy: 0.5729 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 389.2045 - accuracy: 0.5727 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 334.4520 - accuracy: 0.5727 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 251.7683 - accuracy: 0.5728 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 397.1134 - accuracy: 0.5726 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 343.9506 - accuracy: 0.5724 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 319.0556 - accuracy: 0.5726 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 290.3779 - accuracy: 0.5729 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 266.7018 - accuracy: 0.5728 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 314.5425 - accuracy: 0.5727 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 219.3029 - accuracy: 0.5729 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 278.4385 - accuracy: 0.5724 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 263.8556 - accuracy: 0.5729 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 181.1331 - accuracy: 0.5729 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 267.3245 - accuracy: 0.5729 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 339.0871 - accuracy: 0.5724 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 280.0597 - accuracy: 0.5727 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 296.2037 - accuracy: 0.5728 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 185.8445 - accuracy: 0.5729 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 277.5656 - accuracy: 0.5729 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 162.5951 - accuracy: 0.5727 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 274.2667 - accuracy: 0.5729 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 212.6815 - accuracy: 0.5731 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 205.9138 - accuracy: 0.5731 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 233.5249 - accuracy: 0.5727 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 214.2519 - accuracy: 0.5729 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 296.1181 - accuracy: 0.5725 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 234.5260 - accuracy: 0.5730 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 268.1577 - accuracy: 0.5727 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 28ms/step - loss: 249.2672 - accuracy: 0.5730 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 230.2313 - accuracy: 0.5727 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 352.7150 - accuracy: 0.5724 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 220.3566 - accuracy: 0.5730 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 171.6691 - accuracy: 0.5731 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 176.6595 - accuracy: 0.5728 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 163.4298 - accuracy: 0.5730 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 189.3575 - accuracy: 0.5731 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 175.4826 - accuracy: 0.5731 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 184.1353 - accuracy: 0.5728 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 167.4608 - accuracy: 0.5728 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 201.2947 - accuracy: 0.5729 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 159.7438 - accuracy: 0.5726 - val_loss: 1.9010 - val_accuracy: 0.5079\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 154.5900 - accuracy: 0.5728 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 119.2048 - accuracy: 0.5729 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 204.2487 - accuracy: 0.5730 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 145.5051 - accuracy: 0.5732 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 147.4421 - accuracy: 0.5728 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 122.2143 - accuracy: 0.5730 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 154.0787 - accuracy: 0.5729 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 76.8894 - accuracy: 0.5729 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 135.2999 - accuracy: 0.5731 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 149.9319 - accuracy: 0.5728 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 97.2736 - accuracy: 0.5728 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 148.1924 - accuracy: 0.5729 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 80.8587 - accuracy: 0.5731 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 126.4393 - accuracy: 0.5731 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 113.0390 - accuracy: 0.5729 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 83.2792 - accuracy: 0.5731 - val_loss: 1.9008 - val_accuracy: 0.5079\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 104.2914 - accuracy: 0.5733 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 104.2441 - accuracy: 0.5731 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 144.9995 - accuracy: 0.5730 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 27.8400 - accuracy: 0.5734 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 74.2156 - accuracy: 0.5732 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 106.2318 - accuracy: 0.5731 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 82.4414 - accuracy: 0.5735 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 57.0898 - accuracy: 0.5733 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 70.5976 - accuracy: 0.5731 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 58.4235 - accuracy: 0.5735 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 92.9708 - accuracy: 0.5733 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 60.4778 - accuracy: 0.5734 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 85.4523 - accuracy: 0.5731 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 59.5520 - accuracy: 0.5734 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 93.2019 - accuracy: 0.5732 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 66.8301 - accuracy: 0.5735 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 64.5945 - accuracy: 0.5732 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 41.7837 - accuracy: 0.5734 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 63.5114 - accuracy: 0.5731 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 82.4326 - accuracy: 0.5734 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 58.4215 - accuracy: 0.5733 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 47.9429 - accuracy: 0.5734 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 84.4428 - accuracy: 0.5733 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 64.9245 - accuracy: 0.5733 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 57.8046 - accuracy: 0.5733 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 80.1987 - accuracy: 0.5734 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 43.1259 - accuracy: 0.5731 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 28ms/step - loss: 32.8507 - accuracy: 0.5733 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 83.3827 - accuracy: 0.5731 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 54.1902 - accuracy: 0.5734 - val_loss: 1.9007 - val_accuracy: 0.5079\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 46.3325 - accuracy: 0.5733 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 31.3313 - accuracy: 0.5734 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 34.1337 - accuracy: 0.5733 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 61.6208 - accuracy: 0.5735 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 54.3575 - accuracy: 0.5732 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 26.3505 - accuracy: 0.5735 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 42.2299 - accuracy: 0.5734 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 49.2514 - accuracy: 0.5732 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 50.1950 - accuracy: 0.5733 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 32.6580 - accuracy: 0.5734 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 47.3226 - accuracy: 0.5733 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 48.8343 - accuracy: 0.5733 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 49.0990 - accuracy: 0.5734 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 63.3872 - accuracy: 0.5734 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 49.5341 - accuracy: 0.5732 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 94.4667 - accuracy: 0.5732 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 32.3765 - accuracy: 0.5735 - val_loss: 1.9004 - val_accuracy: 0.5079\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 22.0451 - accuracy: 0.5735 - val_loss: 1.9001 - val_accuracy: 0.5079\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 60.2433 - accuracy: 0.5734 - val_loss: 1.9006 - val_accuracy: 0.5079\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 42.2136 - accuracy: 0.5733 - val_loss: 1.8999 - val_accuracy: 0.5079\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 69.3675 - accuracy: 0.5733 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 63.8775 - accuracy: 0.5735 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 48.9552 - accuracy: 0.5734 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 30.3689 - accuracy: 0.5734 - val_loss: 1.9000 - val_accuracy: 0.5079\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 47.6033 - accuracy: 0.5734 - val_loss: 1.9002 - val_accuracy: 0.5079\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 22.2916 - accuracy: 0.5734 - val_loss: 1.9005 - val_accuracy: 0.5079\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 40.4372 - accuracy: 0.5736 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 48.6422 - accuracy: 0.5734 - val_loss: 1.9003 - val_accuracy: 0.5079\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 14.8722 - accuracy: 0.5735 - val_loss: 1.9001 - val_accuracy: 0.5079\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='mean_absolute_error',metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(x_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Analyzing learning curve for the modified model we \n",
    "can see that we are we managed tosee that there is\n",
    "a drop in training loss over epochs, secondly, \n",
    "we seem to replace overfitting with a slight underfit.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = x_train.shape[1], activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "],name=\"Batchnorm\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 2s 43ms/step - loss: 2.2601 - accuracy: 0.4359 - val_loss: 143.4128 - val_accuracy: 0.5079\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.8337 - accuracy: 0.5498 - val_loss: 102.2262 - val_accuracy: 0.5079\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.8044 - accuracy: 0.5628 - val_loss: 36.7366 - val_accuracy: 0.5079\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.7856 - accuracy: 0.5687 - val_loss: 27.6137 - val_accuracy: 0.5079\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.7720 - accuracy: 0.5708 - val_loss: 14.5585 - val_accuracy: 0.5079\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7618 - accuracy: 0.5719 - val_loss: 5.8051 - val_accuracy: 0.5079\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 1.7552 - accuracy: 0.5731 - val_loss: 4.4455 - val_accuracy: 0.5079\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7511 - accuracy: 0.5733 - val_loss: 4.4309 - val_accuracy: 0.5079\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7477 - accuracy: 0.5734 - val_loss: 2.2411 - val_accuracy: 0.5079\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7416 - accuracy: 0.5736 - val_loss: 2.2045 - val_accuracy: 0.5079\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7489 - accuracy: 0.5735 - val_loss: 2.1024 - val_accuracy: 0.5079\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7441 - accuracy: 0.5736 - val_loss: 2.1188 - val_accuracy: 0.5079\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7363 - accuracy: 0.5736 - val_loss: 1.9823 - val_accuracy: 0.5079\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7427 - accuracy: 0.5736 - val_loss: 2.0312 - val_accuracy: 0.5079\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.7293 - accuracy: 0.5736 - val_loss: 1.9967 - val_accuracy: 0.5079\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7278 - accuracy: 0.5736 - val_loss: 1.9179 - val_accuracy: 0.5079\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7306 - accuracy: 0.5736 - val_loss: 1.9958 - val_accuracy: 0.5079\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7343 - accuracy: 0.5736 - val_loss: 1.9503 - val_accuracy: 0.5079\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7496 - accuracy: 0.5736 - val_loss: 1.9436 - val_accuracy: 0.5079\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7306 - accuracy: 0.5736 - val_loss: 1.9290 - val_accuracy: 0.5079\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7261 - accuracy: 0.5736 - val_loss: 2.0329 - val_accuracy: 0.5079\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7387 - accuracy: 0.5736 - val_loss: 1.9987 - val_accuracy: 0.5079\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7360 - accuracy: 0.5736 - val_loss: 1.9592 - val_accuracy: 0.5079\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7315 - accuracy: 0.5736 - val_loss: 1.9233 - val_accuracy: 0.5079\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7240 - accuracy: 0.5736 - val_loss: 1.9282 - val_accuracy: 0.5079\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7232 - accuracy: 0.5736 - val_loss: 1.9173 - val_accuracy: 0.5079\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7361 - accuracy: 0.5736 - val_loss: 1.9423 - val_accuracy: 0.5079\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7206 - accuracy: 0.5736 - val_loss: 2.1883 - val_accuracy: 0.4195\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 1.7246 - accuracy: 0.5736 - val_loss: 2.0807 - val_accuracy: 0.5079\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 1.7179 - accuracy: 0.5736 - val_loss: 1.9717 - val_accuracy: 0.5079\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 1.7246 - accuracy: 0.5736 - val_loss: 2.2158 - val_accuracy: 0.2575\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_674338/4074349914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.compile(optimizer=optimizer, \n\u001b[1;32m      3\u001b[0m             loss='mean_absolute_error',metrics=['accuracy'])\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='mean_absolute_error',metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(x_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding network with an additional hidden layer with 1024 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = x_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "],name=\"Larger_network\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 3s 61ms/step - loss: 2.0381 - accuracy: 0.4779 - val_loss: 11.5687 - val_accuracy: 0.5079\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.8258 - accuracy: 0.5503 - val_loss: 84.6785 - val_accuracy: 0.5079\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7919 - accuracy: 0.5611 - val_loss: 44.6250 - val_accuracy: 0.5079\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7732 - accuracy: 0.5679 - val_loss: 31.1050 - val_accuracy: 0.5079\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7649 - accuracy: 0.5700 - val_loss: 12.1359 - val_accuracy: 0.5079\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7588 - accuracy: 0.5715 - val_loss: 2.3036 - val_accuracy: 0.5079\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7503 - accuracy: 0.5719 - val_loss: 5.9144 - val_accuracy: 0.5079\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7455 - accuracy: 0.5730 - val_loss: 1.9899 - val_accuracy: 0.5079\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7397 - accuracy: 0.5733 - val_loss: 2.0529 - val_accuracy: 0.5079\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7377 - accuracy: 0.5734 - val_loss: 2.3762 - val_accuracy: 0.5079\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7463 - accuracy: 0.5732 - val_loss: 2.4418 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7388 - accuracy: 0.5732 - val_loss: 2.0587 - val_accuracy: 0.5079\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7303 - accuracy: 0.5736 - val_loss: 1.9976 - val_accuracy: 0.5079\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7325 - accuracy: 0.5736 - val_loss: 1.9880 - val_accuracy: 0.5079\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7302 - accuracy: 0.5736 - val_loss: 1.9878 - val_accuracy: 0.5079\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7262 - accuracy: 0.5736 - val_loss: 1.9119 - val_accuracy: 0.5079\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7285 - accuracy: 0.5736 - val_loss: 1.9845 - val_accuracy: 0.5079\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.7265 - accuracy: 0.5736 - val_loss: 1.9438 - val_accuracy: 0.5079\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7310 - accuracy: 0.5736 - val_loss: 1.9891 - val_accuracy: 0.5079\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7282 - accuracy: 0.5736 - val_loss: 1.9077 - val_accuracy: 0.5079\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7219 - accuracy: 0.5736 - val_loss: 1.9321 - val_accuracy: 0.5079\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7198 - accuracy: 0.5736 - val_loss: 1.9122 - val_accuracy: 0.5079\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7185 - accuracy: 0.5736 - val_loss: 1.9323 - val_accuracy: 0.5079\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7173 - accuracy: 0.5736 - val_loss: 1.9326 - val_accuracy: 0.5079\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7168 - accuracy: 0.5736 - val_loss: 1.9244 - val_accuracy: 0.5079\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7145 - accuracy: 0.5736 - val_loss: 1.9162 - val_accuracy: 0.5079\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7142 - accuracy: 0.5736 - val_loss: 1.9180 - val_accuracy: 0.5079\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7160 - accuracy: 0.5736 - val_loss: 1.9166 - val_accuracy: 0.5079\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 2s 60ms/step - loss: 1.7125 - accuracy: 0.5736 - val_loss: 1.9115 - val_accuracy: 0.5079\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7113 - accuracy: 0.5736 - val_loss: 1.9133 - val_accuracy: 0.5079\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7134 - accuracy: 0.5736 - val_loss: 1.9411 - val_accuracy: 0.5079\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.7125 - accuracy: 0.5736 - val_loss: 2.0043 - val_accuracy: 0.5079\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7123 - accuracy: 0.5736 - val_loss: 1.9578 - val_accuracy: 0.5079\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7119 - accuracy: 0.5736 - val_loss: 1.9377 - val_accuracy: 0.5079\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7104 - accuracy: 0.5736 - val_loss: 1.9126 - val_accuracy: 0.5079\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7129 - accuracy: 0.5736 - val_loss: 1.9070 - val_accuracy: 0.5079\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7154 - accuracy: 0.5736 - val_loss: 1.9292 - val_accuracy: 0.5079\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7093 - accuracy: 0.5736 - val_loss: 1.9322 - val_accuracy: 0.5079\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7103 - accuracy: 0.5736 - val_loss: 1.9390 - val_accuracy: 0.5079\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7091 - accuracy: 0.5736 - val_loss: 1.9250 - val_accuracy: 0.5079\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7088 - accuracy: 0.5736 - val_loss: 1.9431 - val_accuracy: 0.5079\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7086 - accuracy: 0.5736 - val_loss: 1.9108 - val_accuracy: 0.5079\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7090 - accuracy: 0.5736 - val_loss: 1.9109 - val_accuracy: 0.5079\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.7082 - accuracy: 0.5736 - val_loss: 1.9133 - val_accuracy: 0.5079\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7093 - accuracy: 0.5736 - val_loss: 1.9169 - val_accuracy: 0.5079\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7081 - accuracy: 0.5736 - val_loss: 1.9105 - val_accuracy: 0.5079\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.7076 - accuracy: 0.5736 - val_loss: 1.9068 - val_accuracy: 0.5079\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7070 - accuracy: 0.5736 - val_loss: 1.9151 - val_accuracy: 0.5079\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7083 - accuracy: 0.5736 - val_loss: 1.9126 - val_accuracy: 0.5079\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7075 - accuracy: 0.5736 - val_loss: 1.9171 - val_accuracy: 0.5079\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7071 - accuracy: 0.5736 - val_loss: 1.9158 - val_accuracy: 0.5079\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7068 - accuracy: 0.5736 - val_loss: 1.9193 - val_accuracy: 0.5079\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7069 - accuracy: 0.5736 - val_loss: 1.9063 - val_accuracy: 0.5079\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7069 - accuracy: 0.5736 - val_loss: 1.9044 - val_accuracy: 0.5079\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7066 - accuracy: 0.5736 - val_loss: 1.9211 - val_accuracy: 0.5079\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7081 - accuracy: 0.5736 - val_loss: 1.9317 - val_accuracy: 0.5079\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7061 - accuracy: 0.5736 - val_loss: 1.9270 - val_accuracy: 0.5079\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7056 - accuracy: 0.5736 - val_loss: 1.9024 - val_accuracy: 0.5079\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7066 - accuracy: 0.5736 - val_loss: 1.9103 - val_accuracy: 0.5079\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7048 - accuracy: 0.5736 - val_loss: 1.9219 - val_accuracy: 0.5079\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7048 - accuracy: 0.5736 - val_loss: 1.9096 - val_accuracy: 0.5079\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7051 - accuracy: 0.5736 - val_loss: 1.9105 - val_accuracy: 0.5079\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7058 - accuracy: 0.5736 - val_loss: 1.9182 - val_accuracy: 0.5079\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7060 - accuracy: 0.5736 - val_loss: 1.9126 - val_accuracy: 0.5079\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7049 - accuracy: 0.5736 - val_loss: 1.9109 - val_accuracy: 0.5079\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7046 - accuracy: 0.5736 - val_loss: 1.9163 - val_accuracy: 0.5079\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7037 - accuracy: 0.5736 - val_loss: 1.9299 - val_accuracy: 0.5079\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7039 - accuracy: 0.5736 - val_loss: 1.9082 - val_accuracy: 0.5079\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7030 - accuracy: 0.5736 - val_loss: 1.9056 - val_accuracy: 0.5079\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.7034 - accuracy: 0.5736 - val_loss: 1.9155 - val_accuracy: 0.5079\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7031 - accuracy: 0.5736 - val_loss: 1.9553 - val_accuracy: 0.5079\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7033 - accuracy: 0.5736 - val_loss: 1.9052 - val_accuracy: 0.5079\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7027 - accuracy: 0.5736 - val_loss: 1.9125 - val_accuracy: 0.5079\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.7029 - accuracy: 0.5736 - val_loss: 1.9135 - val_accuracy: 0.5079\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7025 - accuracy: 0.5736 - val_loss: 1.9197 - val_accuracy: 0.5079\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7033 - accuracy: 0.5736 - val_loss: 1.9065 - val_accuracy: 0.5079\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7032 - accuracy: 0.5736 - val_loss: 1.9080 - val_accuracy: 0.5079\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7025 - accuracy: 0.5736 - val_loss: 1.9087 - val_accuracy: 0.5079\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7025 - accuracy: 0.5736 - val_loss: 1.9069 - val_accuracy: 0.5079\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7022 - accuracy: 0.5736 - val_loss: 1.9191 - val_accuracy: 0.5079\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7022 - accuracy: 0.5736 - val_loss: 1.9069 - val_accuracy: 0.5079\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.7020 - accuracy: 0.5736 - val_loss: 1.9142 - val_accuracy: 0.5079\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.7018 - accuracy: 0.5736 - val_loss: 1.9057 - val_accuracy: 0.5079\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.7034 - accuracy: 0.5736 - val_loss: 1.9104 - val_accuracy: 0.5079\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7035 - accuracy: 0.5736 - val_loss: 1.9041 - val_accuracy: 0.5079\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 1.7014 - accuracy: 0.5736 - val_loss: 1.9115 - val_accuracy: 0.5079\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 1.7026 - accuracy: 0.5736 - val_loss: 1.9058 - val_accuracy: 0.5079\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7021 - accuracy: 0.5736 - val_loss: 1.9081 - val_accuracy: 0.5079\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7019 - accuracy: 0.5736 - val_loss: 1.9152 - val_accuracy: 0.5079\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7018 - accuracy: 0.5736 - val_loss: 1.9107 - val_accuracy: 0.5079\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7008 - accuracy: 0.5736 - val_loss: 1.9058 - val_accuracy: 0.5079\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7018 - accuracy: 0.5736 - val_loss: 1.9019 - val_accuracy: 0.5079\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7029 - accuracy: 0.5736 - val_loss: 1.9078 - val_accuracy: 0.5079\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7006 - accuracy: 0.5736 - val_loss: 1.9088 - val_accuracy: 0.5079\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7008 - accuracy: 0.5736 - val_loss: 1.9053 - val_accuracy: 0.5079\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7010 - accuracy: 0.5736 - val_loss: 1.9066 - val_accuracy: 0.5079\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7008 - accuracy: 0.5736 - val_loss: 1.9468 - val_accuracy: 0.5079\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7011 - accuracy: 0.5736 - val_loss: 1.9212 - val_accuracy: 0.5079\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.7006 - accuracy: 0.5736 - val_loss: 1.9144 - val_accuracy: 0.5079\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7011 - accuracy: 0.5736 - val_loss: 1.9108 - val_accuracy: 0.5079\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7003 - accuracy: 0.5736 - val_loss: 1.9349 - val_accuracy: 0.5079\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.7002 - accuracy: 0.5736 - val_loss: 1.9089 - val_accuracy: 0.5079\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.7006 - accuracy: 0.5736 - val_loss: 1.9060 - val_accuracy: 0.5079\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7002 - accuracy: 0.5736 - val_loss: 1.9084 - val_accuracy: 0.5079\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6995 - accuracy: 0.5736 - val_loss: 1.9158 - val_accuracy: 0.5079\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6997 - accuracy: 0.5736 - val_loss: 1.9121 - val_accuracy: 0.5079\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6996 - accuracy: 0.5736 - val_loss: 1.9131 - val_accuracy: 0.5079\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7002 - accuracy: 0.5736 - val_loss: 1.9023 - val_accuracy: 0.5079\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.7007 - accuracy: 0.5736 - val_loss: 1.9164 - val_accuracy: 0.5079\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7004 - accuracy: 0.5736 - val_loss: 1.9219 - val_accuracy: 0.5079\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6996 - accuracy: 0.5736 - val_loss: 1.9037 - val_accuracy: 0.5079\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6992 - accuracy: 0.5736 - val_loss: 1.9117 - val_accuracy: 0.5079\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6995 - accuracy: 0.5736 - val_loss: 1.9223 - val_accuracy: 0.5079\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6993 - accuracy: 0.5736 - val_loss: 1.9069 - val_accuracy: 0.5079\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6994 - accuracy: 0.5736 - val_loss: 1.9034 - val_accuracy: 0.5079\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6996 - accuracy: 0.5736 - val_loss: 1.9037 - val_accuracy: 0.5079\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6994 - accuracy: 0.5736 - val_loss: 1.9078 - val_accuracy: 0.5079\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.7000 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6989 - accuracy: 0.5736 - val_loss: 1.9346 - val_accuracy: 0.5079\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6992 - accuracy: 0.5736 - val_loss: 1.9057 - val_accuracy: 0.5079\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6991 - accuracy: 0.5736 - val_loss: 1.9170 - val_accuracy: 0.5079\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6994 - accuracy: 0.5736 - val_loss: 1.9405 - val_accuracy: 0.5079\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6991 - accuracy: 0.5736 - val_loss: 1.9081 - val_accuracy: 0.5079\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6993 - accuracy: 0.5736 - val_loss: 1.9370 - val_accuracy: 0.5079\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6987 - accuracy: 0.5736 - val_loss: 1.9029 - val_accuracy: 0.5079\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6988 - accuracy: 0.5736 - val_loss: 1.9077 - val_accuracy: 0.5079\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6987 - accuracy: 0.5736 - val_loss: 1.9026 - val_accuracy: 0.5079\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6988 - accuracy: 0.5736 - val_loss: 1.9095 - val_accuracy: 0.5079\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6985 - accuracy: 0.5736 - val_loss: 1.9545 - val_accuracy: 0.5079\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6984 - accuracy: 0.5736 - val_loss: 1.9234 - val_accuracy: 0.5079\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6987 - accuracy: 0.5736 - val_loss: 1.9081 - val_accuracy: 0.5079\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6986 - accuracy: 0.5736 - val_loss: 1.9021 - val_accuracy: 0.5079\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6983 - accuracy: 0.5736 - val_loss: 1.9277 - val_accuracy: 0.5079\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6986 - accuracy: 0.5736 - val_loss: 1.9152 - val_accuracy: 0.5079\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6980 - accuracy: 0.5736 - val_loss: 1.9144 - val_accuracy: 0.5079\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6985 - accuracy: 0.5736 - val_loss: 1.9095 - val_accuracy: 0.5079\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6987 - accuracy: 0.5736 - val_loss: 1.9143 - val_accuracy: 0.5079\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6995 - accuracy: 0.5736 - val_loss: 1.9074 - val_accuracy: 0.5079\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6983 - accuracy: 0.5736 - val_loss: 1.9112 - val_accuracy: 0.5079\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6980 - accuracy: 0.5736 - val_loss: 1.9065 - val_accuracy: 0.5079\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6981 - accuracy: 0.5736 - val_loss: 1.9066 - val_accuracy: 0.5079\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6979 - accuracy: 0.5736 - val_loss: 1.9045 - val_accuracy: 0.5079\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6984 - accuracy: 0.5736 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6976 - accuracy: 0.5736 - val_loss: 1.9044 - val_accuracy: 0.5079\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6975 - accuracy: 0.5736 - val_loss: 1.9038 - val_accuracy: 0.5079\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6975 - accuracy: 0.5736 - val_loss: 1.9074 - val_accuracy: 0.5079\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6979 - accuracy: 0.5736 - val_loss: 1.9058 - val_accuracy: 0.5079\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6976 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6972 - accuracy: 0.5736 - val_loss: 1.9102 - val_accuracy: 0.5079\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6975 - accuracy: 0.5736 - val_loss: 1.9011 - val_accuracy: 0.5079\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6975 - accuracy: 0.5736 - val_loss: 1.9152 - val_accuracy: 0.5079\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6971 - accuracy: 0.5736 - val_loss: 1.9100 - val_accuracy: 0.5079\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6973 - accuracy: 0.5736 - val_loss: 1.9019 - val_accuracy: 0.5079\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6976 - accuracy: 0.5736 - val_loss: 1.9064 - val_accuracy: 0.5079\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6972 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6973 - accuracy: 0.5736 - val_loss: 1.9018 - val_accuracy: 0.5079\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6972 - accuracy: 0.5736 - val_loss: 1.9016 - val_accuracy: 0.5079\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6970 - accuracy: 0.5736 - val_loss: 1.9042 - val_accuracy: 0.5079\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6973 - accuracy: 0.5736 - val_loss: 1.9261 - val_accuracy: 0.5079\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6971 - accuracy: 0.5736 - val_loss: 1.9276 - val_accuracy: 0.5079\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6972 - accuracy: 0.5736 - val_loss: 1.9044 - val_accuracy: 0.5079\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.6970 - accuracy: 0.5736 - val_loss: 1.9083 - val_accuracy: 0.5079\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6968 - accuracy: 0.5736 - val_loss: 1.9014 - val_accuracy: 0.5079\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6970 - accuracy: 0.5736 - val_loss: 1.9118 - val_accuracy: 0.5079\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6970 - accuracy: 0.5736 - val_loss: 1.9175 - val_accuracy: 0.5079\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9084 - val_accuracy: 0.5079\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6970 - accuracy: 0.5736 - val_loss: 1.9103 - val_accuracy: 0.5079\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 2s 59ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9036 - val_accuracy: 0.5079\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9009 - val_accuracy: 0.5079\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9044 - val_accuracy: 0.5079\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9060 - val_accuracy: 0.5079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6967 - accuracy: 0.5736 - val_loss: 1.9041 - val_accuracy: 0.5079\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6967 - accuracy: 0.5736 - val_loss: 1.9153 - val_accuracy: 0.5079\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6972 - accuracy: 0.5736 - val_loss: 1.9026 - val_accuracy: 0.5079\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6966 - accuracy: 0.5736 - val_loss: 1.9084 - val_accuracy: 0.5079\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9128 - val_accuracy: 0.5079\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9028 - val_accuracy: 0.5079\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 2s 54ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9019 - val_accuracy: 0.5079\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6961 - accuracy: 0.5736 - val_loss: 1.9103 - val_accuracy: 0.5079\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6965 - accuracy: 0.5736 - val_loss: 1.9065 - val_accuracy: 0.5079\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6964 - accuracy: 0.5736 - val_loss: 1.9085 - val_accuracy: 0.5079\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 1.6964 - accuracy: 0.5736 - val_loss: 1.9031 - val_accuracy: 0.5079\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6961 - accuracy: 0.5736 - val_loss: 1.9049 - val_accuracy: 0.5079\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9124 - val_accuracy: 0.5079\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 2s 55ms/step - loss: 1.6961 - accuracy: 0.5736 - val_loss: 1.9457 - val_accuracy: 0.5079\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6963 - accuracy: 0.5736 - val_loss: 1.9207 - val_accuracy: 0.5079\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 2s 57ms/step - loss: 1.6960 - accuracy: 0.5736 - val_loss: 1.9025 - val_accuracy: 0.5079\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 2s 56ms/step - loss: 1.6962 - accuracy: 0.5736 - val_loss: 1.9089 - val_accuracy: 0.5079\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 1.6958 - accuracy: 0.5736 - val_loss: 1.9024 - val_accuracy: 0.5079\n",
      "Epoch 190/200\n",
      " 3/30 [==>...........................] - ETA: 1s - loss: 1.7386 - accuracy: 0.5706"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_674338/4074349914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.compile(optimizer=optimizer, \n\u001b[1;32m      3\u001b[0m             loss='mean_absolute_error',metrics=['accuracy'])\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='mean_absolute_error',metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(x_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NUS_test2",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
